{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeCrd1WXjL8Q"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "OgfuF1_xiAWC",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell is tagged `parameters`\n",
    "# Input\n",
    "input_metanetx_tsv = \"metanetx.train.tsv.gz\"\n",
    "input_emolecules_tsv = \"emolecules.tsv.gz\"\n",
    "\n",
    "# Output\n",
    "outdir = \".\"\n",
    "\n",
    "# Systems\n",
    "threads = 12\n",
    "workdir = \"/data/tmp\" # None (use system)\n",
    "\n",
    "# Fixed parameters\n",
    "nbits = 2048\n",
    "allHsExplicit = False\n",
    "boundary_bonds = False\n",
    "map_root = True\n",
    "rooted_smiles = False\n",
    "\n",
    "RADIUS = [0, 1, 2, 3, 4, 5, 6]\n",
    "ECFP_NBITS = [512, 1024, 2048]\n",
    "SOURCES = [\"metanetx\", \"emolecules\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1GKaoPqjN-n"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVDKvtTdjA6p"
   },
   "outputs": [],
   "source": [
    "# missing deps: seaborn pandarallel ipywidgets scikit-bio\n",
    "import collections\n",
    "import csv\n",
    "import glob\n",
    "import gzip\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import sqlite3\n",
    "import tempfile\n",
    "import time\n",
    "import shutil\n",
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import skbio\n",
    "from pandarallel import pandarallel\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from rdkit import RDLogger\n",
    "from signature.Signature import MoleculeSignature\n",
    "from signature.signature_alphabet import SignatureAlphabet, signature_from_smiles\n",
    "\n",
    "\n",
    "RDLogger.DisableLog(\"rdApp.*\")\n",
    "pandarallel.initialize(nb_workers=threads, progress_bar=True)\n",
    "if workdir is None:\n",
    "    workdir = tempfile.mkdtemp()\n",
    "else:\n",
    "    os.makedirs(workdir, exist_ok=True)\n",
    "workdir_sqlite = os.path.join(workdir, \"sqlite\")\n",
    "os.makedirs(workdir_sqlite, exist_ok=True)\n",
    "os.environ[\"SQLITE_TMPDIR\"] = workdir_sqlite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkLDrb36jQmC"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1k1zWuMejHo_"
   },
   "outputs": [],
   "source": [
    "# SQL\n",
    "def number_records(cursor, table: str) -> int:\n",
    "    cursor.execute('SELECT COUNT(*) FROM %s' % (table,))\n",
    "    res = cursor.fetchone()\n",
    "    if res:\n",
    "        return res[0]\n",
    "    return -1\n",
    "\n",
    "def fetch_in_batches(cursor, batch_size=1000):\n",
    "    while True:\n",
    "        rows = cursor.fetchmany(batch_size)\n",
    "        if not rows:\n",
    "            break\n",
    "        for row in rows:\n",
    "            yield row\n",
    "\n",
    "def file_size(path:str):\n",
    "    file_size_bytes = os.path.getsize(path)\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "    return file_size_mb\n",
    "\n",
    "def vacuum(source_path: str, tempdir: str):\n",
    "    print(\"Start Vacuum\")\n",
    "    source_conn = sqlite3.connect(source_path)\n",
    "    fdb = tempfile.NamedTemporaryFile(dir=tempdir, suffix=\".db\")\n",
    "    target_conn = sqlite3.connect(fdb.name)\n",
    "\n",
    "    with target_conn:\n",
    "        for line in source_conn.iterdump():\n",
    "            if line.strip():\n",
    "                target_conn.execute(line)\n",
    "    source_conn.close()\n",
    "    target_conn.close()\n",
    "\n",
    "    file_in_size = file_size(path=source_path)\n",
    "    file_out_size = file_size(path=fdb.name)\n",
    "\n",
    "    shutil.copyfile(fdb.name, source_path)\n",
    "    print(\"Gain of size (Mb):\", file_in_size - file_out_size)\n",
    "\n",
    "# Molecules\n",
    "def read_tsv(path: str) -> Dict:\n",
    "    csvfile = gzip.open(path, \"rt\", newline=\"\") if path.endswith(\".gz\") else open(path, \"r\", newline==\"\")\n",
    "    reader = csv.DictReader(csvfile, delimiter=\"\\t\")\n",
    "    for ix, row in enumerate(reader):\n",
    "        ident = str(ix)\n",
    "        smi = row[\"SMILES\"]\n",
    "        if smi and len(smi) > 0:\n",
    "            yield dict(id=ident, smiles=smi)\n",
    "    csvfile.close()\n",
    "\n",
    "def compute_hash(label: str, size: int = 64):\n",
    "    return hashlib.shake_256(label.encode(\"utf8\")).hexdigest(size)\n",
    "\n",
    "def ecfp(smi: str, radius: int, nbits: int) -> str:\n",
    "    fpgen = AllChem.GetMorganGenerator(radius=radius, fpSize=nbits, includeChirality=True)\n",
    "    fp = fpgen.GetCountFingerprint(AllChem.MolFromSmiles(smi))\n",
    "    secfp = \"\".join([str(x) for x in fp.ToList()])\n",
    "    return compute_hash(secfp)\n",
    "\n",
    "# Signature\n",
    "def compute_sig(smi: str, alphabet):\n",
    "    sig, _, _ = signature_from_smiles(smi, alphabet)\n",
    "    return compute_hash(sig)\n",
    "\n",
    "def compute_fragment(smi: str, radius: int, boundary_bonds: bool, map_root: bool, rooted_smiles: bool, nbits: int):\n",
    "    ms = MoleculeSignature(\n",
    "        Chem.MolFromSmiles(smi),\n",
    "        radius=radius,\n",
    "        boundary_bonds=boundary_bonds,\n",
    "        map_root=map_root,\n",
    "        rooted_smiles=rooted_smiles,\n",
    "        nbits=nbits,\n",
    "    )\n",
    "    return \",\".join([compute_hash(x) for x in ms.to_list()])\n",
    "\n",
    "# Compute data for fragment, signature, ECFP\n",
    "def compute_data(df: pd.DataFrame, boundary_bonds: bool, map_root: bool, rooted_smiles: bool, nbits: int, radius: int) -> pd.DataFrame:\n",
    "    df.drop_duplicates(subset=[\"smiles\"], inplace=True)\n",
    "\n",
    "    print(\"\\t\", \"Compute fragments\")\n",
    "    df[\"fragment\"] = df[\"smiles\"].parallel_apply(compute_fragment, args=(radius, boundary_bonds, map_root, rooted_smiles, nbits,))\n",
    "\n",
    "    print(\"\\t\", \"Compute signature\")\n",
    "    alphabet = SignatureAlphabet(radius=radius, nBits=nbits)\n",
    "    df[\"sig\"] = df[\"smiles\"].parallel_apply(compute_sig, args=(alphabet,))\n",
    "\n",
    "    for ecfp_nbits in ECFP_NBITS:\n",
    "        print(\"\\t\", \"Ecfp nbits:\", ecfp_nbits)\n",
    "        df[f\"ecfp_{ecfp_nbits}\"] = df[\"smiles\"].parallel_apply(ecfp, args=(radius, ecfp_nbits))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Xv3lEWvjZcA"
   },
   "source": [
    "## Create databases of signatures/fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bo8t81i9jYOf"
   },
   "outputs": [],
   "source": [
    "for step_radiux, radius in enumerate(RADIUS):\n",
    "    print(\"Radius:\", radius)\n",
    "    output_database_db = os.path.join(outdir, f\"radius-{radius}.db\")\n",
    "    conn = sqlite3.connect(output_database_db)\n",
    "\n",
    "    def compute_file(conn, origin: str, path: str, tempdir: str, radius: int):\n",
    "        # Origin: \"emolecules\" | \"metanetx\"\n",
    "        print(\"Compute:\", origin)\n",
    "        tempdir_compute = os.path.join(tempdir, origin)\n",
    "        os.makedirs(tempdir_compute, exist_ok=True)\n",
    "\n",
    "        if step_radius == 0:\n",
    "            max_size = 1e6\n",
    "            data = []\n",
    "            current_item = 0\n",
    "            current_file = 0\n",
    "\n",
    "            # Parse and split file for memory purposes\n",
    "            for ix, rec in enumerate(read_tsv(path=path)):\n",
    "                if current_item > max_size - 1:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    df.to_csv(os.path.join(tempdir_compute, \".\".join([origin, str(current_file), \"raw\", \"csv\", \"gz\"])), index=False)\n",
    "                    data = []\n",
    "                    current_item = 0\n",
    "                    current_file += 1\n",
    "                data.append(rec)\n",
    "                current_item += 1\n",
    "\n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(os.path.join(tempdir_compute, \".\".join([origin, str(current_file), \"raw\", \"csv\", \"gz\"])), index=False)\n",
    "            current_file += 1\n",
    "\n",
    "        # Compute signature\n",
    "        cursor = conn.cursor()\n",
    "        for ix, filename in enumerate(glob.glob(os.path.join(tempdir_compute, f\"{origin}.*.raw.csv.gz\"))):\n",
    "            print(\"\\t\", \"Batch:\", ix)\n",
    "            df = pd.read_csv(filename)\n",
    "            df = compute_data(\n",
    "                df=df,\n",
    "                boundary_bonds=boundary_bonds,\n",
    "                map_root=map_root,\n",
    "                rooted_smiles=rooted_smiles,\n",
    "                nbits=nbits,\n",
    "                radius=radius,\n",
    "            )\n",
    "            if df.empty:\n",
    "                continue\n",
    "            df.to_sql(origin, conn, if_exists=\"append\", index=False, chunksize=int(1e4))\n",
    "            del df\n",
    "\n",
    "        # Clean up\n",
    "        if step_radius == len(RADIUS - 1):\n",
    "            shutil.rmtree(tempdir_compute, ignore_errors=True)\n",
    "\n",
    "    # MetaNetX\n",
    "    compute_file(conn=conn, origin=\"metanetx\", path=input_metanetx_tsv, tempdir=workdir, radius=radius)\n",
    "\n",
    "    # eMolecules\n",
    "    compute_file(conn=conn, origin=\"emolecules\", path=input_emolecules_tsv, tempdir=workdir, radius=radius)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    vacuum(source_path=output_database_db, tempdir=workdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bymkU_hmwXw"
   },
   "source": [
    "## Create metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yo1L1hq7iC-b"
   },
   "outputs": [],
   "source": [
    "# Define colnames expected in tables\n",
    "COLNAMES = [\"smiles\", \"sig\"] + [f\"ecfp_{ecfp_nbits}\" for ecfp_nbits in ECFP_NBITS]\n",
    "\n",
    "for radius in RADIUS\n",
    "    print(\"Radius:\", radius)\n",
    "\n",
    "    # Define files\n",
    "    output_database_db = os.path.join(outdir, f\"radius-{radius}.db\")\n",
    "    output_degeneracy_csv = os.path.join(outdir, f\"degeneracy.radius-{radius}.csv\")\n",
    "    output_diversity_csv = os.path.join(outdir, f\"diversity.radius-{radius}.csv\")\n",
    "    output_alphabet_csv = os.path.join(outdir, f\"alphabet.radius-{radius}.csv\")\n",
    "\n",
    "    if not os.path.isfile(output_database_db):\n",
    "        print(f\"Output database: {output_database_db} not found -> Skip\")\n",
    "        continue\n",
    "    if os.path.isfile(output_degeneracy_csv) and os.path.isfile(output_diversity_csv) and os.path.isfile(output_alphabet_csv):\n",
    "        continue\n",
    "\n",
    "    # Total records\n",
    "    print(\"Total records\")\n",
    "    conn = sqlite3.connect(output_database_db)\n",
    "    cursor = conn.cursor()\n",
    "    total_records = {}\n",
    "    for source in SOURCES:\n",
    "        total_records[source] = number_records(cursor=cursor, table=source)\n",
    "    conn.close()\n",
    "\n",
    "    # Degeneracy\n",
    "    if not os.path.isfile(output_degeneracy_csv):\n",
    "        print(\"Degeneracy\")\n",
    "        conn = sqlite3.connect(output_database_db)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        def compute_degeneracy(cursor, source: str, colname: str, radius: int):\n",
    "            print(\"Query - start:\", source, colname)\n",
    "            start = time.time()\n",
    "            collision = collections.Counter() # key: occurences, value: number of occurences\n",
    "            table_name = source\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {table_name} GROUP BY {colname}\")\n",
    "            for res in fetch_in_batches(cursor=cursor):\n",
    "                collision.update([res[0]])\n",
    "            print(\"Query - end\", time.time() - start)\n",
    "            return dict(collision)\n",
    "\n",
    "        datas = []\n",
    "        for source, colname in itertools.product(SOURCES, COLNAMES):\n",
    "            print(\"Compute for:\", source, \" - \", colname)\n",
    "            data_deg = compute_degeneracy(cursor=cursor, source=source, colname=colname, radius=radius)\n",
    "            for occurence, value in data_deg.items():\n",
    "                data = dict(item=colname, source=source, kind=\"occurence\", occurence=occurence, value=value, radius=radius)\n",
    "                datas.append(data)\n",
    "            datas.append(dict(item=colname, source=source, kind=\"total\", value=total_records[source], radius=radius))\n",
    "        df = pd.DataFrame(datas)\n",
    "        df.to_csv(output_degeneracy_csv, index=False)\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "    # Diversity\n",
    "    if not os.path.isfile(output_diversity_csv):\n",
    "        print(\"Diversity\")\n",
    "        conn = sqlite3.connect(output_database_sql)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Build depth\n",
    "        METRICS = [\n",
    "            \"ace\",\n",
    "            \"chao1\",\n",
    "            \"dominance\",\n",
    "            \"gini_index\",\n",
    "            \"hill\",\n",
    "            \"inv_simpson\",\n",
    "            \"observed_features\",\n",
    "            \"pielou_e\",\n",
    "            \"simpson\",\n",
    "            \"simpson_d\",\n",
    "            \"simpson_e\",\n",
    "            \"shannon\",\n",
    "        ]\n",
    "\n",
    "        # Build depth\n",
    "        depths = np.logspace(1, 7, num=7, base=10, dtype=int)\n",
    "        for depth in depths[::-1]:\n",
    "            depths = np.append(depths, int(depth / 2))\n",
    "        depths = np.sort(depths)\n",
    "        depths = depths[:-1] # rm 10e6\n",
    "\n",
    "        datas = []\n",
    "        for seed, (batch, source, depth) in enumerate(itertools.product(range(10), SOURCES, depths)):\n",
    "\n",
    "            start = time.time()\n",
    "            data = dict(batch=batch, source=source, depth=depth, seed=seed, radius=radius)\n",
    "\n",
    "            print(\"\\t\", \"Deal with:\", data)\n",
    "            # Select Indices\n",
    "            total_record = total_records[source]\n",
    "            if total_record < depth:\n",
    "                continue\n",
    "            if total_record == depth:\n",
    "                indices = list(range(total_record))\n",
    "            else:\n",
    "                rng = np.random.default_rng(seed)\n",
    "                indices = np.sort(rng.choice(total_record, size=depth, replace=False, shuffle=False))\n",
    "            cur_index = 0\n",
    "\n",
    "            # Build taxons\n",
    "            print(\"\\t\", \"Build taxons\")\n",
    "            counter = collections.Counter()\n",
    "            cursor.execute(f\"SELECT fragment FROM {source}\")\n",
    "            for ix, res in enumerate(fetch_in_batches(cursor=cursor)):\n",
    "                if cur_index >= depth:\n",
    "                    break\n",
    "                if ix == indices[cur_index]:\n",
    "                    counter.update(res[0].split(\",\"))\n",
    "                    cur_index += 1\n",
    "            assert cur_index == depth\n",
    "            taxons = list(counter.values())\n",
    "\n",
    "            # Aggressive clean up\n",
    "            del counter\n",
    "\n",
    "            # Compute Alpha div\n",
    "            print(\"\\t\", \"Compute alpha div\")\n",
    "            for metric in METRICS:\n",
    "                value = None\n",
    "                try:\n",
    "                    value = skbio.diversity.alpha_diversity(metric, taxons).iloc[0]\n",
    "                except:\n",
    "                    pass\n",
    "                data_metric = copy.deepcopy(data)\n",
    "                data_metric[\"metric\"] = metric\n",
    "                data_metric[\"value\"] = value\n",
    "                datas.append(data_metric)\n",
    "\n",
    "            print(\"\\t\", \"End, time:\", time.time() - start)\n",
    "\n",
    "            # Aggressive clean up\n",
    "            del taxons\n",
    "\n",
    "        df = pd.DataFrame(datas)\n",
    "        # Save\n",
    "        df.to_csv(output_diversity_csv, index=False)\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "    # Alphabet\n",
    "    if not os.path.isfile(output_alphabet_csv):\n",
    "        print(\"Alphabet\")\n",
    "        conn = sqlite3.connect(output_database_db)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        for source in SOURCES:\n",
    "            alphabets = set()\n",
    "            cursor.execute(f\"SELECT fragment FROM {source}\")\n",
    "            datas = []\n",
    "            for res in fetch_in_batches(cursor=cursor), total=total_records[source]:\n",
    "                fragments = res[0].split(\",\")\n",
    "                nb_added = 0\n",
    "                for fragment in fragments:\n",
    "                    if fragment in alphabets:\n",
    "                        continue\n",
    "                    alphabets.add(fragment)\n",
    "                    nb_added += 1\n",
    "                is_added = int(nb_added > 0)\n",
    "                datas.append(dict(is_added=is_added, nb_fragment_added=nb_added, nb_fragment=len(fragments), source=source))\n",
    "        df = pd.DataFrame(datas)\n",
    "        df.index.name = \"step\"\n",
    "\n",
    "        df.to_csv(output_alphabet_csv)\n",
    "\n",
    "        conn.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
