{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Analysis of eMolecules and MetaNetX alphabets in terms of size and diversity**\n",
    "\n",
    "In this notebook, we investigate eMolecules and MetaNetX alphabet. The \n",
    "calculated metrics are presented in Figure 2 of the manuscript, as well\n",
    "as Figures S1 and S3 of the supplementary material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeCrd1WXjL8Q"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "OgfuF1_xiAWC",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell is tagged `parameters`\n",
    "# input\n",
    "input_metanetx_tsv = \"metanetx.train.tsv.gz\"\n",
    "input_emolecules_tsv = \"emolecules.train.tsv.gz\"\n",
    "input_chembl_tsv = \"smiles_chembl_stereo_500.tsv.gz\"\n",
    "\n",
    "outdir = \"/data/signature/results\"\n",
    "\n",
    "\n",
    "threads = 14\n",
    "tempdir = \"/data/tmp\"\n",
    "\n",
    "# Fixed parameters\n",
    "nbits = 2048\n",
    "\n",
    "#radius = 2\n",
    "ECFP_NBITS = [512, 1024, 2048]\n",
    "SOURCES = [\"metanetx\", \"emolecules\", \"chembl\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1GKaoPqjN-n"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVDKvtTdjA6p"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import gzip\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import multiprocessing\n",
    "import sqlite3\n",
    "import tempfile\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import RDLogger\n",
    "from molsig.Signature import MoleculeSignature\n",
    "\n",
    "RDLogger.DisableLog(\"rdApp.*\")\n",
    "pandarallel.initialize(nb_workers=threads, progress_bar=True)\n",
    "\n",
    "if tempdir:\n",
    "    os.makedirs(tempdir, exist_ok=True)\n",
    "    tempdir_sqlite = os.path.join(tempdir, \"sqlite\")\n",
    "    os.makedirs(tempdir_sqlite, exist_ok=True)\n",
    "    os.environ[\"SQLITE_TMPDIR\"] = tempdir_sqlite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkLDrb36jQmC"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1k1zWuMejHo_"
   },
   "outputs": [],
   "source": [
    "# Molecules\n",
    "def read_tsv(path: str) -> Dict:\n",
    "    with gzip.open(path, \"rt\", newline=\"\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile, delimiter=\"\\t\")\n",
    "        for ix, row in enumerate(reader):\n",
    "            ident = str(ix)\n",
    "            smi = row[\"SMILES\"]\n",
    "            if smi and len(smi) > 0:\n",
    "                 yield dict(id=ident, smiles=smi)\n",
    "\n",
    "def compute_hash(label: str, size: int = 64):\n",
    "    return hashlib.shake_256(label.encode(\"utf8\")).hexdigest(size)\n",
    "\n",
    "def ecfp(smi: str, radius: int, nbits: int) -> str:\n",
    "    fpgen = AllChem.GetMorganGenerator(radius=radius, fpSize=nbits, includeChirality=True)\n",
    "    # fp = fpgen.GetFingerprint(mol)  # returns a bit vector (value 1 or 0)\n",
    "    fp = fpgen.GetCountFingerprint(AllChem.MolFromSmiles(smi))\n",
    "    secfp = \"\".join([str(x) for x in fp.ToList()])\n",
    "    return compute_hash(secfp)\n",
    "\n",
    "# Signature\n",
    "def compute_signature(smi: str, radius: int):\n",
    "    try:\n",
    "        ms = MoleculeSignature(\n",
    "            Chem.MolFromSmiles(smi),\n",
    "            radius=radius,\n",
    "            nbits=0,\n",
    "            use_stereo=False,\n",
    "        )\n",
    "        return compute_hash(ms.to_string())\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def compute_fragment(smi: str, radius: int, nbits: int):\n",
    "    # Compute the molecule signature\n",
    "    try:\n",
    "        ms = MoleculeSignature(\n",
    "            Chem.MolFromSmiles(smi),\n",
    "            radius=radius,\n",
    "            nbits=nbits,\n",
    "        )\n",
    "        return \",\".join([compute_hash(x) for x in ms.to_list()])\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def compute_data(df: pd.DataFrame, nbits: int, radius: int) -> pd.DataFrame:\n",
    "    df.drop_duplicates(subset=[\"smiles\"], inplace=True)\n",
    "\n",
    "    print(\"\\t\", \"Compute fragments\")\n",
    "    df[\"fragment\"] = df[\"smiles\"].parallel_apply(compute_fragment, args=(radius, nbits,))\n",
    "\n",
    "    print(\"\\t\", \"Compute signature\")\n",
    "    df[\"sig\"] = df[\"smiles\"].parallel_apply(compute_signature, args=(radius, nbits,))\n",
    "\n",
    "    df = df[~pd.isna(df[\"fragment\"])]\n",
    "    df = df[~pd.isna(df[\"sig\"])]\n",
    "\n",
    "    for ecfp_nbits in ECFP_NBITS:\n",
    "        print(\"\\t\", \"Ecfp nbits:\", ecfp_nbits)\n",
    "        df[f\"ecfp_{ecfp_nbits}\"] = df[\"smiles\"].parallel_apply(ecfp, args=(radius, ecfp_nbits))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Xv3lEWvjZcA"
   },
   "source": [
    "## Create databases of signatures/fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bo8t81i9jYOf"
   },
   "outputs": [],
   "source": [
    "for radius in range(7):\n",
    "    print(\"Radius:\", radius)\n",
    "    output_database_sql = os.path.join(outdir, f\"radius-{radius}.sql\")\n",
    "    \n",
    "    conn = sqlite3.connect(output_database_sql)\n",
    "\n",
    "    def compute_file(conn, origin: str, path: str, tempdir: str, radius: int):\n",
    "        # Origin, either \"emolecules\", \"metanetx\", chembl\n",
    "        print(\"Compute:\", origin)\n",
    "        tempdir_compute = os.path.join(tempdir, origin)\n",
    "        os.makedirs(tempdir_compute, exist_ok=True)\n",
    "        if radius == 0:\n",
    "            # Init\n",
    "            max_size = 1e6\n",
    "            data = []\n",
    "            current_item = 0\n",
    "            current_file = 0\n",
    "\n",
    "            # Parse and split file for memory purposes\n",
    "            for ix, rec in enumerate(read_tsv(path=path)):\n",
    "                if current_item > max_size - 1:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    df.to_csv(os.path.join(tempdir_compute, \".\".join([origin, str(current_file), \"raw\", \"csv\", \"gz\"])), index=False)\n",
    "                    data = []\n",
    "                    current_item = 0\n",
    "                    current_file += 1\n",
    "                data.append(rec)\n",
    "                current_item += 1\n",
    "\n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(os.path.join(tempdir_compute, \".\".join([origin, str(current_file), \"raw\", \"csv\", \"gz\"])), index=False)\n",
    "            current_file += 1\n",
    "        # Compute signature\n",
    "        cursor = conn.cursor()\n",
    "        for ix, filename in enumerate(glob.glob(os.path.join(tempdir_compute, f\"{origin}.*.raw.csv.gz\"))):\n",
    "            print(\"\\t\", \"Batch:\", ix)\n",
    "            df = pd.read_csv(filename)\n",
    "            df = compute_data(\n",
    "                df=df,\n",
    "                nbits=nbits,\n",
    "                radius=radius,\n",
    "            )\n",
    "            if df.empty:\n",
    "                continue\n",
    "                \n",
    "            df.to_sql(origin, conn, if_exists=\"append\", index=False, chunksize=int(1e4))\n",
    "            del df\n",
    "\n",
    "    # Compute\n",
    "    for origin, path in zip([\"metanetx\", \"emolecules\", \"chembl\"], [input_metanetx_tsv, input_emolecules_tsv, input_chembl_tsv]):\n",
    "        compute_file(conn=conn, origin=origin, path=path, tempdir=tempdir, radius=radius)\n",
    "    \n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bymkU_hmwXw"
   },
   "source": [
    "## Create metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yo1L1hq7iC-b"
   },
   "outputs": [],
   "source": [
    "# Define colnames expected in tables\n",
    "COLNAMES = [\"smiles\", \"sig\"] + [f\"ecfp_{ecfp_nbits}\" for ecfp_nbits in ECFP_NBITS]\n",
    "\n",
    "for radius in RADIUS\n",
    "    print(\"Radius:\", radius)\n",
    "\n",
    "    # Define files\n",
    "    output_database_db = os.path.join(outdir, f\"radius-{radius}.db\")\n",
    "    output_degeneracy_csv = os.path.join(outdir, f\"degeneracy.radius-{radius}.csv\")\n",
    "    output_diversity_csv = os.path.join(outdir, f\"diversity.radius-{radius}.csv\")\n",
    "    output_alphabet_csv = os.path.join(outdir, f\"alphabet.radius-{radius}.csv\")\n",
    "\n",
    "    if not os.path.isfile(output_database_db):\n",
    "        print(f\"Output database: {output_database_db} not found -> Skip\")\n",
    "        continue\n",
    "    if os.path.isfile(output_degeneracy_csv) and os.path.isfile(output_diversity_csv) and os.path.isfile(output_alphabet_csv):\n",
    "        continue\n",
    "\n",
    "    # Total records\n",
    "    print(\"Total records\")\n",
    "    conn = sqlite3.connect(output_database_db)\n",
    "    cursor = conn.cursor()\n",
    "    total_records = {}\n",
    "    for source in SOURCES:\n",
    "        total_records[source] = number_records(cursor=cursor, table=source)\n",
    "    conn.close()\n",
    "\n",
    "    # Degeneracy\n",
    "    if not os.path.isfile(output_degeneracy_csv):\n",
    "        print(\"Degeneracy\")\n",
    "        conn = sqlite3.connect(output_database_db)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        def compute_degeneracy(cursor, source: str, colname: str, radius: int):\n",
    "            print(\"Query - start:\", source, colname)\n",
    "            start = time.time()\n",
    "            collision = collections.Counter() # key: occurences, value: number of occurences\n",
    "            table_name = source\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {table_name} GROUP BY {colname}\")\n",
    "            for res in fetch_in_batches(cursor=cursor):\n",
    "                collision.update([res[0]])\n",
    "            print(\"Query - end\", time.time() - start)\n",
    "            return dict(collision)\n",
    "\n",
    "        datas = []\n",
    "        for source, colname in itertools.product(SOURCES, COLNAMES):\n",
    "            print(\"Compute for:\", source, \" - \", colname)\n",
    "            data_deg = compute_degeneracy(cursor=cursor, source=source, colname=colname, radius=radius)\n",
    "            for occurence, value in data_deg.items():\n",
    "                data = dict(item=colname, source=source, kind=\"occurence\", occurence=occurence, value=value, radius=radius)\n",
    "                datas.append(data)\n",
    "            datas.append(dict(item=colname, source=source, kind=\"total\", value=total_records[source], radius=radius))\n",
    "        df = pd.DataFrame(datas)\n",
    "        df.to_csv(output_degeneracy_csv, index=False)\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "    # Diversity\n",
    "    if not os.path.isfile(output_diversity_csv):\n",
    "        print(\"Diversity\")\n",
    "        conn = sqlite3.connect(output_database_sql)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Build depth\n",
    "        METRICS = [\n",
    "            \"ace\",\n",
    "            \"chao1\",\n",
    "            \"dominance\",\n",
    "            \"gini_index\",\n",
    "            \"hill\",\n",
    "            \"inv_simpson\",\n",
    "            \"observed_features\",\n",
    "            \"pielou_e\",\n",
    "            \"simpson\",\n",
    "            \"simpson_d\",\n",
    "            \"simpson_e\",\n",
    "            \"shannon\",\n",
    "        ]\n",
    "\n",
    "        # Build depth\n",
    "        depths = np.logspace(1, 7, num=7, base=10, dtype=int)\n",
    "        for depth in depths[::-1]:\n",
    "            depths = np.append(depths, int(depth / 2))\n",
    "        depths = np.sort(depths)\n",
    "        depths = depths[:-1] # rm 10e6\n",
    "\n",
    "        datas = []\n",
    "        for seed, (batch, source, depth) in enumerate(itertools.product(range(10), SOURCES, depths)):\n",
    "\n",
    "            start = time.time()\n",
    "            data = dict(batch=batch, source=source, depth=depth, seed=seed, radius=radius)\n",
    "\n",
    "            print(\"\\t\", \"Deal with:\", data)\n",
    "            # Select Indices\n",
    "            total_record = total_records[source]\n",
    "            if total_record < depth:\n",
    "                continue\n",
    "            if total_record == depth:\n",
    "                indices = list(range(total_record))\n",
    "            else:\n",
    "                rng = np.random.default_rng(seed)\n",
    "                indices = np.sort(rng.choice(total_record, size=depth, replace=False, shuffle=False))\n",
    "            cur_index = 0\n",
    "\n",
    "            # Build taxons\n",
    "            print(\"\\t\", \"Build taxons\")\n",
    "            counter = collections.Counter()\n",
    "            cursor.execute(f\"SELECT fragment FROM {source}\")\n",
    "            for ix, res in enumerate(fetch_in_batches(cursor=cursor)):\n",
    "                if cur_index >= depth:\n",
    "                    break\n",
    "                if ix == indices[cur_index]:\n",
    "                    counter.update(res[0].split(\",\"))\n",
    "                    cur_index += 1\n",
    "            assert cur_index == depth\n",
    "            taxons = list(counter.values())\n",
    "\n",
    "            # Aggressive clean up\n",
    "            del counter\n",
    "\n",
    "            # Compute Alpha div\n",
    "            print(\"\\t\", \"Compute alpha div\")\n",
    "            for metric in METRICS:\n",
    "                value = None\n",
    "                try:\n",
    "                    value = skbio.diversity.alpha_diversity(metric, taxons).iloc[0]\n",
    "                except:\n",
    "                    pass\n",
    "                data_metric = copy.deepcopy(data)\n",
    "                data_metric[\"metric\"] = metric\n",
    "                data_metric[\"value\"] = value\n",
    "                datas.append(data_metric)\n",
    "\n",
    "            print(\"\\t\", \"End, time:\", time.time() - start)\n",
    "\n",
    "            # Aggressive clean up\n",
    "            del taxons\n",
    "\n",
    "        df = pd.DataFrame(datas)\n",
    "        # Save\n",
    "        df.to_csv(output_diversity_csv, index=False)\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "    # Alphabet\n",
    "    if not os.path.isfile(output_alphabet_csv):\n",
    "        print(\"Alphabet\")\n",
    "        conn = sqlite3.connect(output_database_db)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        for source in SOURCES:\n",
    "            alphabets = set()\n",
    "            cursor.execute(f\"SELECT fragment FROM {source}\")\n",
    "            datas = []\n",
    "            for res in fetch_in_batches(cursor=cursor), total=total_records[source]:\n",
    "                fragments = res[0].split(\",\")\n",
    "                nb_added = 0\n",
    "                for fragment in fragments:\n",
    "                    if fragment in alphabets:\n",
    "                        continue\n",
    "                    alphabets.add(fragment)\n",
    "                    nb_added += 1\n",
    "                is_added = int(nb_added > 0)\n",
    "                datas.append(dict(is_added=is_added, nb_fragment_added=nb_added, nb_fragment=len(fragments), source=source))\n",
    "        df = pd.DataFrame(datas)\n",
    "        df.index.name = \"step\"\n",
    "\n",
    "        df.to_csv(output_alphabet_csv)\n",
    "\n",
    "        conn.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
