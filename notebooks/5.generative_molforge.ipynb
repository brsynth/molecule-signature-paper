{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Cross-comparison from our and MolForge's generative models**\n",
    "\n",
    "In this notebook, we evaluate the performance of our model on the MolForge dataset (10k molecules), and the performance of the MolForge model on the MetaNetX and eMolecules datasets (2 x 10k molecules).\n",
    "\n",
    "Results are used to populate table 3 in the main text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1 — Pass MolForge's test set through generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 — Init & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tduigou/miniforge3/envs/signature-paper/lib/python3.11/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/tduigou/miniforge3/envs/signature-paper/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <EB3FF92A-5EB1-3EE8-AF8B-5923C1265422> /Users/tduigou/miniforge3/envs/signature-paper/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/Users/tduigou/miniforge3/envs/signature-paper/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/tduigou/miniforge3/envs/signature-paper/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/tduigou/miniforge3/envs/signature-paper/lib/python3.11/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/tduigou/miniforge3/envs/signature-paper/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import RDLogger  # for disabling RDKit warnings\n",
    "\n",
    "from paper.dataset.utils import (\n",
    "    assign_stereo,\n",
    "    mol_from_smiles,\n",
    "    mol_to_smiles,\n",
    "    mol_to_ecfp,\n",
    "    ecfp_to_string,\n",
    "    tanimoto,\n",
    ")\n",
    "import handy\n",
    "from handy import mol_to_ecfp_molforge, ecfp_to_string_molforge\n",
    "from paper.learning import predict\n",
    "from paper.learning.configure import Config\n",
    "\n",
    "# Logging --------------------------------------------------------------------\n",
    "\n",
    "RDLogger.DisableLog(\"rdApp.error\")\n",
    "RDLogger.DisableLog('rdApp.warning')\n",
    "\n",
    "\n",
    "# Utils ------------------------------------------------------------------------\n",
    "\n",
    "def remove_spaces(s):\n",
    "    return \"\".join(s.split()).strip()\n",
    "\n",
    "\n",
    "def mol_to_ecfp_string(mol):\n",
    "    return ecfp_to_string(mol_to_ecfp(mol))\n",
    "\n",
    "\n",
    "def mol_from_smiles_full_stereo(smiles):\n",
    "    \"\"\"Convert SMILES to RDKit Mol object with full stereo information.\"\"\"\n",
    "    try:\n",
    "        mol = mol_from_smiles(smiles, clear_stereo=False)\n",
    "        return assign_stereo(mol)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def mol_from_smiles_no_stereo(smiles):\n",
    "    \"\"\"Convert SMILES to RDKit Mol object without stereo information.\"\"\"\n",
    "    try:\n",
    "        return mol_from_smiles(smiles, clear_stereo=True)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def mol_from_smiles_with_exception(smiles):\n",
    "    try:\n",
    "        return mol_from_smiles(smiles)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Settings --------------------------------------------------------------------\n",
    "\n",
    "BASE_DIR = Path().resolve().parent\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"molforge\"\n",
    "OUT_DIR = BASE_DIR / \"notebooks\" / \"molforge\" / \"case-1\"\n",
    "FILENAME = \"ECFP4.smiles.test\"\n",
    "\n",
    "# Prediction settings\n",
    "CONFIG = Config(\n",
    "    model_path= BASE_DIR / \"data\" / \"models\" / \"finetuned.ckpt\",\n",
    "    model_source_tokenizer= BASE_DIR / \"data\" / \"tokens\" / \"ECFP.model\",\n",
    "    model_target_tokenizer= BASE_DIR / \"data\" / \"tokens\" / \"SMILES.model\",\n",
    "    pred_mode=\"beam\",  # \"greedy\" or \"beam\"\n",
    "    pred_batch_size=10,\n",
    "    pred_beam_size=1,  # >1 for beam search, not used for greedy\n",
    "    pred_max_rows=-1,  # -1 means no limit\n",
    ")\n",
    "CONFIG.device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 — Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation -------------------------------------------------------------\n",
    "\n",
    "for stereo_case in [\"raw\", \"full_stereo\", \"no_stereo\"]:\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    out_case_dir = OUT_DIR / stereo_case\n",
    "    out_case_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Output filename\n",
    "    out_case_filename = f\"data.tsv\"\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_csv(DATA_DIR / FILENAME, sep=\"\\t\", header=None)\n",
    "    data.rename(columns={0: \"Query SMILES\", 1: \"Query ECFP\"}, inplace=True)\n",
    "\n",
    "    # Remove spaces in SMILES\n",
    "    data[\"Query SMILES\"] = data[\"Query SMILES\"].apply(remove_spaces)\n",
    "\n",
    "    if stereo_case == \"raw\":\n",
    "        # Use stereo state as provided in the dataset\n",
    "        data[\"Query Mol\"] = data[\"Query SMILES\"].apply(mol_from_smiles_with_exception)\n",
    "        data[\"Query SMILES\"] = data[\"Query Mol\"].apply(mol_to_smiles)\n",
    "        data[\"Query ECFP\"] = data[\"Query Mol\"].apply(mol_to_ecfp_string)\n",
    "    \n",
    "    elif stereo_case == \"full_stereo\":\n",
    "        # Use a fully specified stereo state\n",
    "        data[\"Query Mol\"] = data[\"Query SMILES\"].apply(mol_from_smiles_full_stereo)\n",
    "        data[\"Query SMILES\"] = data[\"Query Mol\"].apply(mol_to_smiles)\n",
    "        data[\"Query ECFP\"] = data[\"Query Mol\"].apply(mol_to_ecfp_string)\n",
    "\n",
    "    elif stereo_case == \"no_stereo\":\n",
    "        # Use a flat stereo state\n",
    "        data[\"Query Mol\"] = data[\"Query SMILES\"].apply(mol_from_smiles_no_stereo)\n",
    "        data[\"Query SMILES\"] = data[\"Query Mol\"].apply(mol_to_smiles)\n",
    "        data[\"Query ECFP\"] = data[\"Query Mol\"].apply(mol_to_ecfp_string)\n",
    "\n",
    "    # Clean\n",
    "    data.drop(columns=[\"Query Mol\"], inplace=True)\n",
    "\n",
    "    # Append a \"Query ID\" column containing the row number for easier reference\n",
    "    data['Query ID'] = range(1, len(data) + 1)\n",
    "\n",
    "    # Reorder columns to have \"Query ID\" first\n",
    "    cols = data.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    data = data[cols]\n",
    "\n",
    "    # Save the processed data\n",
    "    data.to_csv(out_case_dir / out_case_filename, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 — Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/tduigou/miniforge3/envs/signature-paper/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1000/1000 [40:37<00:00,  0.41it/s]\n"
     ]
    }
   ],
   "source": [
    "stereo_case = \"no_stereo\"  # Change this to \"raw\" or \"full_stereo\" or \"no_stereo\" as needed\n",
    "work_dir = OUT_DIR / stereo_case\n",
    "data_filename = f\"data.tsv\"\n",
    "out_filename = f\"results.raw.tsv\"\n",
    "\n",
    "\n",
    "# Prediction -------------------------------------------------------------------\n",
    "\n",
    "# Load the data for prediction\n",
    "data = pd.read_csv(work_dir / data_filename, sep=\"\\t\")\n",
    "\n",
    "# Truncate data according to the prediction limit\n",
    "data = data.iloc[:CONFIG.pred_max_rows]\n",
    "\n",
    "# Predict\n",
    "with handy.Timer() as timer:\n",
    "    results = predict.run(CONFIG, query_data=data[\"Query ECFP\"].values)\n",
    "\n",
    "# Append the average time per query to each row on the results\n",
    "# This is a trick to get per query time without modifying the predict.run function\n",
    "avg_time_per_query = timer.elapsed / len(results)\n",
    "results[\"Time Elapsed\"] = avg_time_per_query\n",
    "\n",
    "# Post-processing --------------------------------------------------------------\n",
    "\n",
    "# Merge results with the original data using the \"Query ID\" column\n",
    "results = pd.merge(data, results, on=\"Query ID\", how=\"left\")\n",
    "assert results[\"Query ECFP_x\"].equals(results[\"Query ECFP_y\"])\n",
    "results.drop(columns=[\"Query ECFP_y\"], inplace=True)\n",
    "results = results.rename(columns={\"Query ECFP_x\": \"Query ECFP\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results -----------------------------------------------------------------\n",
    "results.to_csv(work_dir / out_filename, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 — Refine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_case = \"no_stereo\"  # Change this to \"raw\" or \"full_stereo\" or \"no_stereo\" as needed\n",
    "work_dir = OUT_DIR / stereo_case\n",
    "results_filename = \"results.raw.tsv\"\n",
    "out_filename = \"results.refined.tsv\"\n",
    "\n",
    "\n",
    "# Load data -------------------------------------------------------------------\n",
    "\n",
    "data = pd.read_csv(work_dir / results_filename, sep=\"\\t\")\n",
    "\n",
    "\n",
    "# Refine data -----------------------------------------------------------------\n",
    "\n",
    "# Let's recompute required information on the Query side\n",
    "data[\"Query Mol\"] = data[\"Query SMILES\"].apply(mol_from_smiles_with_exception)\n",
    "data[\"Query Counted ECFP Object\"] = data[\"Query Mol\"].apply(mol_to_ecfp)\n",
    "data[\"Query Counted ECFP\"] = data[\"Query Counted ECFP Object\"].apply(ecfp_to_string)\n",
    "data[\"Query Binary ECFP Object\"] = data[\"Query Mol\"].apply(mol_to_ecfp_molforge)\n",
    "data[\"Query Binary ECFP\"] = data[\"Query Binary ECFP Object\"].apply(ecfp_to_string_molforge)\n",
    "\n",
    "# Now let's populate back the prediction side\n",
    "data[\"Predicted Prob\"] = data[\"Predicted Log Prob\"].apply(np.exp)\n",
    "data[\"Predicted Mol\"] = data[\"Predicted SMILES\"].apply(mol_from_smiles_with_exception)\n",
    "data[\"Predicted Canonic SMILES\"] = data[\"Predicted Mol\"].apply(mol_to_smiles)\n",
    "data[\"Predicted Counted ECFP Object\"] = data[\"Predicted Mol\"].apply(mol_to_ecfp)\n",
    "data[\"Predicted Counted ECFP\"] = data[\"Predicted Counted ECFP Object\"].apply(ecfp_to_string)\n",
    "data[\"Predicted Binary ECFP Object\"] = data[\"Predicted Mol\"].apply(mol_to_ecfp_molforge)\n",
    "data[\"Predicted Binary ECFP\"] = data[\"Predicted Binary ECFP Object\"].apply(ecfp_to_string_molforge)\n",
    "\n",
    "# Now let's check for Mol validity\n",
    "data[\"SMILES Syntaxically Valid\"] = data[\"Predicted Mol\"].notnull()\n",
    "\n",
    "# Get the No Stereo version\n",
    "data[\"Query Mol No Stereo\"] = data[\"Query SMILES\"].apply(mol_from_smiles_no_stereo)\n",
    "data[\"Query SMILES No Stereo\"] = data[\"Query Mol No Stereo\"].apply(mol_to_smiles)\n",
    "data[\"Query Counted ECFP Object No Stereo\"] = data[\"Query Mol No Stereo\"].apply(mol_to_ecfp)\n",
    "data[\"Query Counted ECFP No Stereo\"] = data[\"Query Counted ECFP Object No Stereo\"].apply(ecfp_to_string)\n",
    "\n",
    "data[\"Predicted Mol No Stereo\"] = data[\"Predicted Canonic SMILES\"].apply(mol_from_smiles_no_stereo)\n",
    "data[\"Predicted SMILES No Stereo\"] = data[\"Predicted Mol No Stereo\"].apply(mol_to_smiles)\n",
    "data[\"Predicted Counted ECFP Object No Stereo\"] = data[\"Predicted Mol No Stereo\"].apply(mol_to_ecfp)\n",
    "data[\"Predicted Counted ECFP No Stereo\"] = data[\"Predicted Counted ECFP Object No Stereo\"].apply(ecfp_to_string)\n",
    "\n",
    "# Now let's check for SMILES equality (with and without stereo)\n",
    "data[\"SMILES Exact Match\"] = data[\"Query SMILES\"] == data[\"Predicted Canonic SMILES\"]\n",
    "data[\"SMILES Exact Match No Stereo\"] = data[\"Query SMILES No Stereo\"] == data[\"Predicted SMILES No Stereo\"]\n",
    "\n",
    "# Now let's check for Tanimoto similarity (with and without stereo)\n",
    "data[\"Tanimoto Counted ECFP\"] = data.apply(lambda x: tanimoto(x[\"Query Counted ECFP Object\"], x[\"Predicted Counted ECFP Object\"]), axis=1)\n",
    "data[\"Tanimoto Counted ECFP No Stereo\"] = data.apply(lambda x: tanimoto(x[\"Query Counted ECFP Object No Stereo\"], x[\"Predicted Counted ECFP Object No Stereo\"]), axis=1)\n",
    "data[\"Tanimoto Binary ECFP\"] = data.apply(lambda x: tanimoto(x[\"Query Binary ECFP Object\"], x[\"Predicted Binary ECFP Object\"]), axis=1)\n",
    "data[\"Tanimoto Counted ECFP Exact Match\"] = data[\"Tanimoto Counted ECFP\"] == 1.0\n",
    "data[\"Tanimoto Counted ECFP Exact Match No Stereo\"] = data[\"Tanimoto Counted ECFP No Stereo\"] == 1.0\n",
    "data[\"Tanimoto Binary ECFP Exact Match\"] = data[\"Tanimoto Binary ECFP\"] == 1.0\n",
    "\n",
    "# Finally export the refined DataFrame\n",
    "cols = [\n",
    "    \"Query ID\",\n",
    "    \"Query SMILES\",\n",
    "    \"Query SMILES No Stereo\",\n",
    "    \"Query Counted ECFP\",\n",
    "    \"Query Counted ECFP No Stereo\",\n",
    "    \"Query Binary ECFP\",\n",
    "    \"Predicted Tokens\",\n",
    "    \"Predicted Log Prob\",\n",
    "    \"Predicted Prob\",\n",
    "    \"Predicted SMILES\",\n",
    "    \"Predicted SMILES No Stereo\",\n",
    "    \"Predicted Counted ECFP\",\n",
    "    \"Predicted Counted ECFP No Stereo\",\n",
    "    \"Predicted Binary ECFP\",\n",
    "    \"Predicted Canonic SMILES\",\n",
    "    \"Tanimoto Counted ECFP\",\n",
    "    \"Tanimoto Counted ECFP No Stereo\",\n",
    "    \"Tanimoto Binary ECFP\",\n",
    "    \"SMILES Exact Match\",\n",
    "    \"SMILES Exact Match No Stereo\",\n",
    "    \"Tanimoto Counted ECFP Exact Match\",\n",
    "    \"Tanimoto Counted ECFP Exact Match No Stereo\",\n",
    "    \"Tanimoto Binary ECFP Exact Match\",\n",
    "    \"SMILES Syntaxically Valid\",\n",
    "    \"Time Elapsed\",\n",
    "]\n",
    "data.to_csv(work_dir / out_filename, sep=\"\\t\", index=False, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 — Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_stereo stats:\n",
      "                                       Stat     Value\n",
      "0                           SMILES Accuracy  0.354835\n",
      "1            Tanimoto Counted ECFP Accuracy  0.371737\n",
      "2                    SMILES Syntax Validity  0.851885\n",
      "3                 SMILES No Stereo Accuracy  0.662066\n",
      "4  Tanimoto Counted ECFP No Stereo Accuracy  0.673267\n",
      "5             Tanimoto Binary ECFP Accuracy  0.673267\n",
      "6                      Average Time Elapsed  0.243924\n",
      "\n",
      "no_stereo uniqueness:\n",
      "  Distinct Molecules per Query  Count\n",
      "0                            0   6282\n",
      "1                            1   3717\n"
     ]
    }
   ],
   "source": [
    "stereo_case = \"no_stereo\"  # Change this to \"raw\" or \"full_stereo\" or \"no_stereo\" as needed\n",
    "top_k = 1\n",
    "work_dir = OUT_DIR / stereo_case\n",
    "results_filename = f\"results.refined.tsv\"\n",
    "\n",
    "# Load data -------------------------------------------------------------------\n",
    "\n",
    "data = pd.read_csv(work_dir / results_filename, sep=\"\\t\")\n",
    "\n",
    "# Summary ---------------------------------------------------------------------\n",
    "summary = handy.get_summary(data, topk=top_k)\n",
    "out_filename = results_filename.replace(\"refined.tsv\", \"summary.tsv\")\n",
    "summary.to_csv(work_dir / out_filename, sep=\"\\t\", index=False)\n",
    "\n",
    "# Statistics -------------------------------------------------------------------\n",
    "stats = handy.get_statistics(data, topk=top_k)\n",
    "out_filename = results_filename.replace(\"refined.tsv\", \"stastitics.tsv\")\n",
    "stats.to_csv(work_dir / out_filename, sep=\"\\t\", index=False)\n",
    "\n",
    "# Uniqueness -------------------------------------------------------------------\n",
    "uniqueness = handy.get_uniqueness(data, topk=top_k)\n",
    "out_filename = results_filename.replace(\"refined.tsv\", \"uniqueness.tsv\")\n",
    "uniqueness.to_csv(work_dir / out_filename, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"{stereo_case} stats:\")\n",
    "print(stats)\n",
    "print()\n",
    "print(f\"{stereo_case} uniqueness:\")\n",
    "print(uniqueness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 — Pass MetaNetX test set through MolForge's model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 — Init & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "from rdkit import RDLogger\n",
    "\n",
    "from paper.dataset.utils import (\n",
    "    mol_from_smiles,\n",
    "    mol_to_smiles,\n",
    "    tanimoto,\n",
    "    mol_to_ecfp,\n",
    "    ecfp_to_string,\n",
    ")\n",
    "from MolForge.utils import pad_or_truncate, pad_id, build_model\n",
    "from MolForge.predict import greedy_search, beam_search, setup\n",
    "from handy import mol_to_ecfp_molforge, ecfp_to_string_molforge\n",
    "\n",
    "# Logging --------------------------------------------------------------------\n",
    "\n",
    "RDLogger.DisableLog(\"rdApp.error\")\n",
    "RDLogger.DisableLog('rdApp.warning')\n",
    "\n",
    "\n",
    "# Utils ------------------------------------------------------------------------\n",
    "\n",
    "def remove_spaces(x):\n",
    "    return x.replace(\" \", \"\")\n",
    "\n",
    "\n",
    "def inference(model, input_sentence, method, args, src_sp, trg_sp, return_attn=False):\n",
    "\n",
    "    tokenized = src_sp.EncodeAsIds(input_sentence)\n",
    "    src_data = torch.LongTensor(pad_or_truncate(tokenized, args.src_seq_len)).unsqueeze(0).to(args.device) # (1, L)\n",
    "    e_mask = (src_data != pad_id).unsqueeze(1).to(args.device) # (1, 1, L)\n",
    "\n",
    "    model.eval()\n",
    "    src_data = model.src_embedding(src_data)\n",
    "    src_data = model.src_positional_encoder(src_data)\n",
    "    e_output = model.encoder(src_data, e_mask) # (1, L, d_model)\n",
    "\n",
    "    if method == 'greedy':\n",
    "        result, attn = greedy_search(model, e_output, e_mask, trg_sp, args.device, True)\n",
    "\n",
    "    elif method == 'beam':\n",
    "        result, attn = beam_search(model, e_output, e_mask, trg_sp, args.device, True)\n",
    "\n",
    "    if return_attn:\n",
    "        return result, attn\n",
    "    \n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "def mol_from_smiles_with_exception(mol):\n",
    "    try:\n",
    "        return mol_from_smiles(mol)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def mol_from_smiles_no_stereo(smiles):\n",
    "    \"\"\"Convert SMILES to RDKit Mol object without stereo information.\"\"\"\n",
    "    try:\n",
    "        return mol_from_smiles(smiles, clear_stereo=True)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Settings --------------------------------------------------------------------\n",
    "\n",
    "BASE_DIR = Path().resolve().parent\n",
    "TEST_FILE = BASE_DIR / 'data' / 'metanetx' / 'splitting' / 'test.tsv'\n",
    "WORK_DIR = BASE_DIR / 'notebooks' / 'molforge' / 'case-2'\n",
    "\n",
    "SOURCE_SP = BASE_DIR / \"data\" / \"molforge\" / \"ECFP4_vocab_sp.model\"\n",
    "TARGET_SP = BASE_DIR / \"data\" / \"molforge\" / \"smiles_vocab_sp.model\"\n",
    "\n",
    "MODEL_PATH = BASE_DIR / \"data\" / \"molforge\" / \"ECFP4_smiles_checkpoint.pth\"\n",
    "METHOD = \"greedy\"\n",
    "MAX_ROWS = 10000\n",
    "\n",
    "ARGS = SimpleNamespace(\n",
    "    fp = \"ECFP4\",\n",
    "    model_type = \"smiles\",\n",
    "    checkpoint = str(MODEL_PATH),\n",
    "    decode = \"greedy\",\n",
    "    src_vocab_size = 2052,\n",
    "    trg_vocab_size = 109,\n",
    "    src_seq_len = 104,\n",
    "    trg_seq_len = 130,\n",
    "    root_dir = BASE_DIR,\n",
    "    src_sp_prefix = str(SOURCE_SP).replace(\".model\", \"\"),\n",
    "    trg_sp_prefix = str(TARGET_SP).replace(\".model\", \"\"),\n",
    "    rank = \"cpu\",\n",
    "    device = \"cpu\",\n",
    ")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Write settings\n",
    "with open(WORK_DIR / \"settings.txt\", \"w\") as f:\n",
    "    f.write(\"BASE_DIR: \" + str(BASE_DIR) + \"\\n\")\n",
    "    f.write(\"TEST_FILE: \" + str(TEST_FILE) + \"\\n\")\n",
    "    f.write(\"WORK_DIR: \" + str(WORK_DIR) + \"\\n\")\n",
    "    f.write(\"MAX_ROWS: \" + str(MAX_ROWS) + \"\\n\")\n",
    "    for key, value in ARGS.__dict__.items():\n",
    "        f.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 — Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filename = \"data.tsv\"\n",
    "\n",
    "# Data preparation -------------------------------------------------------------\n",
    "\n",
    "# Load test data\n",
    "data = pd.read_csv(TEST_FILE, sep='\\t', nrows=MAX_ROWS)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data.drop(columns=['SMILES_0', 'SIGNATURE', 'SIGNATURE_MORGANS'], inplace=True)\n",
    "\n",
    "# Rename few columns for more clarity\n",
    "data.rename(columns={'SMILES': 'Query SMILES', 'ECFP': 'Query ECFP'}, inplace=True)\n",
    "\n",
    "# Compute ECFP compatible with the MolForge \n",
    "data[\"Query SMILES\"] = data[\"Query SMILES\"].apply(lambda x: x.replace(\" \", \"\"))\n",
    "data[\"Query Mol\"] = data[\"Query SMILES\"].apply(mol_from_smiles)\n",
    "data[\"Query ECFP Object\"] = data[\"Query Mol\"].apply(mol_to_ecfp_molforge)\n",
    "data[\"Query ECFP\"] = data[\"Query ECFP Object\"].apply(ecfp_to_string_molforge)\n",
    "\n",
    "# Clean up the DataFrame\n",
    "data.drop(columns=[\"Query Mol\", \"Query ECFP Object\"], inplace=True)\n",
    "\n",
    "# Save processed data\n",
    "data.to_csv(WORK_DIR / out_filename, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 — Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of src vocab is 2052 and that of trg vocab is 109.\n",
      "Loading checkpoint... ECFP4 smiles\n",
      "Processing 1 / 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tduigou/projects/2024__RetroSynthesis/MolForge/MolForge/predict.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device(args.rank))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 101 / 10000\n",
      "Processing 201 / 10000\n",
      "Processing 301 / 10000\n",
      "Processing 401 / 10000\n",
      "Processing 501 / 10000\n",
      "Processing 601 / 10000\n",
      "Processing 701 / 10000\n",
      "Processing 801 / 10000\n",
      "Processing 901 / 10000\n",
      "Processing 1001 / 10000\n",
      "Processing 1101 / 10000\n",
      "Processing 1201 / 10000\n",
      "Processing 1301 / 10000\n",
      "Processing 1401 / 10000\n",
      "Processing 1501 / 10000\n",
      "Processing 1601 / 10000\n",
      "Processing 1701 / 10000\n",
      "Processing 1801 / 10000\n",
      "Processing 1901 / 10000\n",
      "Processing 2001 / 10000\n",
      "Processing 2101 / 10000\n",
      "Processing 2201 / 10000\n",
      "Processing 2301 / 10000\n",
      "Processing 2401 / 10000\n",
      "Processing 2501 / 10000\n",
      "Processing 2601 / 10000\n",
      "Processing 2701 / 10000\n",
      "Processing 2801 / 10000\n",
      "Processing 2901 / 10000\n",
      "Processing 3001 / 10000\n",
      "Processing 3101 / 10000\n",
      "Processing 3201 / 10000\n",
      "Processing 3301 / 10000\n",
      "Processing 3401 / 10000\n",
      "Processing 3501 / 10000\n",
      "Processing 3601 / 10000\n",
      "Processing 3701 / 10000\n",
      "Processing 3801 / 10000\n",
      "Processing 3901 / 10000\n",
      "Processing 4001 / 10000\n",
      "Processing 4101 / 10000\n",
      "Processing 4201 / 10000\n",
      "Processing 4301 / 10000\n",
      "Processing 4401 / 10000\n",
      "Processing 4501 / 10000\n",
      "Processing 4601 / 10000\n",
      "Processing 4701 / 10000\n",
      "Processing 4801 / 10000\n",
      "Processing 4901 / 10000\n",
      "Processing 5001 / 10000\n",
      "Processing 5101 / 10000\n",
      "Processing 5201 / 10000\n",
      "Processing 5301 / 10000\n",
      "Processing 5401 / 10000\n",
      "Processing 5501 / 10000\n",
      "Processing 5601 / 10000\n",
      "Processing 5701 / 10000\n",
      "Processing 5801 / 10000\n",
      "Processing 5901 / 10000\n",
      "Processing 6001 / 10000\n",
      "Processing 6101 / 10000\n",
      "Processing 6201 / 10000\n",
      "Processing 6301 / 10000\n",
      "Processing 6401 / 10000\n",
      "Processing 6501 / 10000\n",
      "Processing 6601 / 10000\n",
      "Processing 6701 / 10000\n",
      "Processing 6801 / 10000\n",
      "Processing 6901 / 10000\n",
      "Processing 7001 / 10000\n",
      "Processing 7101 / 10000\n",
      "Processing 7201 / 10000\n",
      "Processing 7301 / 10000\n",
      "Processing 7401 / 10000\n",
      "Processing 7501 / 10000\n",
      "Processing 7601 / 10000\n",
      "Processing 7701 / 10000\n",
      "Processing 7801 / 10000\n",
      "Processing 7901 / 10000\n",
      "Processing 8001 / 10000\n",
      "Processing 8101 / 10000\n",
      "Processing 8201 / 10000\n",
      "Processing 8301 / 10000\n",
      "Processing 8401 / 10000\n",
      "Processing 8501 / 10000\n",
      "Processing 8601 / 10000\n",
      "Processing 8701 / 10000\n",
      "Processing 8801 / 10000\n",
      "Processing 8901 / 10000\n",
      "Processing 9001 / 10000\n",
      "Processing 9101 / 10000\n",
      "Processing 9201 / 10000\n",
      "Processing 9301 / 10000\n",
      "Processing 9401 / 10000\n",
      "Processing 9501 / 10000\n",
      "Processing 9601 / 10000\n",
      "Processing 9701 / 10000\n",
      "Processing 9801 / 10000\n",
      "Processing 9901 / 10000\n"
     ]
    }
   ],
   "source": [
    "data_filename = \"data.tsv\"\n",
    "results_filename = \"results.raw.tsv\"\n",
    "\n",
    "# Set up model -------------------------------------------------------------\n",
    "\n",
    "# Load SentencePiece models\n",
    "src_sp = spm.SentencePieceProcessor()\n",
    "src_sp.Load(str(SOURCE_SP))\n",
    "trg_sp = spm.SentencePieceProcessor()\n",
    "trg_sp.Load(str(TARGET_SP))\n",
    "\n",
    "# Load model\n",
    "model = setup(build_model(ARGS).to(ARGS.device), ARGS.checkpoint, ARGS)\n",
    "\n",
    "# Results DataFrame --------------------------------------------------------\n",
    "results = pd.DataFrame(columns=[\"Query ID\", \"Query SMILES\", \"Query ECFP\", \"Predicted SMILES\", \"Time Elapsed\"])\n",
    "\n",
    "# Prediction -------------------------------------------------------------------\n",
    "\n",
    "data = pd.read_csv(WORK_DIR / data_filename, sep=\"\\t\")\n",
    "\n",
    "# Loop over data ECFP\n",
    "for idx, row in data.iterrows():\n",
    "\n",
    "    if idx >= MAX_ROWS:\n",
    "        break\n",
    "    \n",
    "    query_id = idx + 1\n",
    "    query_smiles = row[\"Query SMILES\"]\n",
    "    query_ecfp = row[\"Query ECFP\"]\n",
    "\n",
    "    # Log progress\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processing {query_id} / {len(data)}\")\n",
    "\n",
    "    with handy.Timer() as timer:\n",
    "        # Perform inference\n",
    "        predi_tokens = inference(model, query_ecfp, METHOD, ARGS, src_sp, trg_sp)\n",
    "\n",
    "    predi_smiles = inference(model, query_ecfp, METHOD, ARGS, src_sp, trg_sp)\n",
    "    query_smiles = query_smiles.replace(\" \", \"\")\n",
    "    predi_smiles = predi_smiles.replace(\" \", \"\")\n",
    "    results.loc[idx] = [query_id, query_smiles, query_ecfp, predi_smiles, timer.elapsed]\n",
    "\n",
    "\n",
    "# Save results -----------------------------------------------------------------\n",
    "results.to_csv(WORK_DIR / results_filename, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 — Refine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filename = \"results.raw.tsv\"\n",
    "out_filename = \"results.refined.tsv\"\n",
    "\n",
    "# Load data -------------------------------------------------------------------\n",
    "\n",
    "data = pd.read_csv(WORK_DIR / results_filename, sep=\"\\t\")\n",
    "\n",
    "\n",
    "# Refine data -----------------------------------------------------------------\n",
    "\n",
    "# Let's recompute required information on the Query side\n",
    "data[\"Query SMILES\"] = data[\"Query SMILES\"].apply(remove_spaces)\n",
    "data[\"Query Mol\"] = data[\"Query SMILES\"].apply(mol_from_smiles_with_exception)\n",
    "data[\"Query Counted ECFP Object\"] = data[\"Query Mol\"].apply(mol_to_ecfp)\n",
    "data[\"Query Counted ECFP\"] = data[\"Query Counted ECFP Object\"].apply(ecfp_to_string)\n",
    "data[\"Query Binary ECFP Object\"] = data[\"Query Mol\"].apply(mol_to_ecfp_molforge)\n",
    "data[\"Query Binary ECFP\"] = data[\"Query Binary ECFP Object\"].apply(ecfp_to_string_molforge)\n",
    "\n",
    "# Now let's populate back the prediction side\n",
    "data[\"Predicted Mol\"] = data[\"Predicted SMILES\"].apply(mol_from_smiles_with_exception)\n",
    "data[\"Predicted Counted ECFP Object\"] = data[\"Predicted Mol\"].apply(mol_to_ecfp)\n",
    "data[\"Predicted Counted ECFP\"] = data[\"Predicted Counted ECFP Object\"].apply(ecfp_to_string)\n",
    "data[\"Predicted Binary ECFP Object\"] = data[\"Predicted Mol\"].apply(mol_to_ecfp_molforge)\n",
    "data[\"Predicted Binary ECFP\"] = data[\"Predicted Binary ECFP Object\"].apply(ecfp_to_string_molforge)\n",
    "data[\"Predicted Canonic SMILES\"] = data[\"Predicted Mol\"].apply(mol_to_smiles)\n",
    "\n",
    "# Now let's check for Mol validity\n",
    "data[\"SMILES Syntaxically Valid\"] = data[\"Predicted Mol\"].notnull()\n",
    "\n",
    "# Now let's check for SMILES equality (with and without stereo)\n",
    "data[\"SMILES Exact Match\"] = data[\"Query SMILES\"] == data[\"Predicted Canonic SMILES\"]\n",
    "\n",
    "# Now let's check for Tanimoto identity (with and without stereo)\n",
    "data[\"Tanimoto Counted ECFP\"] = data.apply(lambda x: tanimoto(x[\"Query Counted ECFP Object\"], x[\"Predicted Counted ECFP Object\"]), axis=1)\n",
    "data[\"Tanimoto Counted ECFP Exact Match\"] = data[\"Tanimoto Counted ECFP\"] == 1.0\n",
    "data[\"Tanimoto Binary ECFP\"] = data.apply(lambda x: tanimoto(x[\"Query Binary ECFP Object\"], x[\"Predicted Binary ECFP Object\"]), axis=1)\n",
    "data[\"Tanimoto Binary ECFP Exact Match\"] = data[\"Tanimoto Binary ECFP\"] == 1.0\n",
    "\n",
    "# Finally export the refined DataFrame\n",
    "cols = [\n",
    "    \"Query ID\",\n",
    "    \"Query SMILES\",\n",
    "    \"Query Counted ECFP\",\n",
    "    \"Query Binary ECFP\",\n",
    "    \"Predicted SMILES\",\n",
    "    \"Predicted Counted ECFP\",\n",
    "    \"Predicted Binary ECFP\",\n",
    "    \"Predicted Canonic SMILES\",\n",
    "    \"Tanimoto Counted ECFP\",\n",
    "    \"Tanimoto Counted ECFP Exact Match\",\n",
    "    \"Tanimoto Binary ECFP\",\n",
    "    \"Tanimoto Binary ECFP Exact Match\",\n",
    "    \"SMILES Exact Match\",\n",
    "    \"SMILES Syntaxically Valid\",\n",
    "    \"Time Elapsed\",\n",
    "]\n",
    "data.to_csv(WORK_DIR / out_filename, sep=\"\\t\", index=False, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 — Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats:\n",
      "                             Stat     Value\n",
      "0                 SMILES Accuracy  0.193200\n",
      "1  Tanimoto Counted ECFP Accuracy  0.204300\n",
      "2          SMILES Syntax Validity  0.975300\n",
      "3   Tanimoto Binary ECFP Accuracy  0.496400\n",
      "4            Average Time Elapsed  1.571109\n",
      "\n",
      "uniqueness:\n",
      "  Distinct Molecules per Query  Count\n",
      "0                            0   7957\n",
      "1                            1   2043\n"
     ]
    }
   ],
   "source": [
    "results_filename = \"results.refined.tsv\"\n",
    "top_k = 1\n",
    "\n",
    "from handy import get_summary, get_statistics, get_uniqueness\n",
    "\n",
    "\n",
    "# Load data -------------------------------------------------------------------\n",
    "\n",
    "data = pd.read_csv(WORK_DIR / results_filename, sep=\"\\t\")\n",
    "\n",
    "# Add dummy \"Predicted Log Prob\" column if not present (for compatibility)\n",
    "if \"Predicted Log Prob\" not in data.columns:\n",
    "    data[\"Predicted Log Prob\"] = 0.0\n",
    "\n",
    "\n",
    "# Summary ---------------------------------------------------------------------\n",
    "\n",
    "summary = get_summary(data, topk=top_k)\n",
    "out_filename = \"results.summary.tsv\"\n",
    "summary.to_csv(WORK_DIR / out_filename, sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "# Statistics -------------------------------------------------------------------\n",
    "\n",
    "stats = get_statistics(data, topk=top_k)\n",
    "out_filename = \"results.statistics.tsv\"\n",
    "stats.to_csv(WORK_DIR / out_filename, sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "# Uniqueness -------------------------------------------------------------------\n",
    "\n",
    "uniqueness = get_uniqueness(data, topk=top_k)\n",
    "out_filename = \"results.uniqueness.tsv\"\n",
    "uniqueness.to_csv(WORK_DIR / out_filename, sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "print(f\"stats:\")\n",
    "print(stats)\n",
    "print()\n",
    "print(f\"uniqueness:\")\n",
    "print(uniqueness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 — Pass eMolecules test set through MolForge's model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 — Init & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "from rdkit import RDLogger\n",
    "\n",
    "import handy\n",
    "from paper.dataset.utils import (\n",
    "    mol_from_smiles,\n",
    "    mol_to_smiles,\n",
    "    tanimoto,\n",
    "    mol_to_ecfp,\n",
    "    ecfp_to_string,\n",
    ")\n",
    "from handy import mol_to_ecfp_molforge, ecfp_to_string_molforge\n",
    "from MolForge.utils import pad_or_truncate, pad_id, build_model\n",
    "from MolForge.predict import greedy_search, beam_search, setup\n",
    "\n",
    "# Logging --------------------------------------------------------------------\n",
    "\n",
    "RDLogger.DisableLog(\"rdApp.error\")\n",
    "RDLogger.DisableLog('rdApp.warning')\n",
    "\n",
    "\n",
    "# Utils ------------------------------------------------------------------------\n",
    "\n",
    "def remove_spaces(x):\n",
    "    return x.replace(\" \", \"\")\n",
    "\n",
    "\n",
    "def inference(model, input_sentence, method, args, src_sp, trg_sp, return_attn=False):\n",
    "\n",
    "    tokenized = src_sp.EncodeAsIds(input_sentence)\n",
    "    src_data = torch.LongTensor(pad_or_truncate(tokenized, args.src_seq_len)).unsqueeze(0).to(args.device) # (1, L)\n",
    "    e_mask = (src_data != pad_id).unsqueeze(1).to(args.device) # (1, 1, L)\n",
    "\n",
    "    model.eval()\n",
    "    src_data = model.src_embedding(src_data)\n",
    "    src_data = model.src_positional_encoder(src_data)\n",
    "    e_output = model.encoder(src_data, e_mask) # (1, L, d_model)\n",
    "\n",
    "    if method == 'greedy':\n",
    "        result, attn = greedy_search(model, e_output, e_mask, trg_sp, args.device, True)\n",
    "\n",
    "    elif method == 'beam':\n",
    "        result, attn = beam_search(model, e_output, e_mask, trg_sp, args.device, True)\n",
    "\n",
    "    if return_attn:\n",
    "        return result, attn\n",
    "    \n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "def mol_from_smiles_with_exception(mol):\n",
    "    try:\n",
    "        return mol_from_smiles(mol)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def mol_from_smiles_no_stereo(smiles):\n",
    "    \"\"\"Convert SMILES to RDKit Mol object without stereo information.\"\"\"\n",
    "    try:\n",
    "        return mol_from_smiles(smiles, clear_stereo=True)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Settings --------------------------------------------------------------------\n",
    "\n",
    "BASE_DIR = Path().resolve().parent\n",
    "TEST_FILE = BASE_DIR / 'data' / 'emolecules' / 'splitting' / 'test.tsv'\n",
    "WORK_DIR = BASE_DIR / 'notebooks' / 'molforge' / 'case-3'\n",
    "\n",
    "SOURCE_SP = BASE_DIR / \"data\" / \"molforge\" / \"ECFP4_vocab_sp.model\"\n",
    "TARGET_SP = BASE_DIR / \"data\" / \"molforge\" / \"smiles_vocab_sp.model\"\n",
    "\n",
    "MODEL_PATH = BASE_DIR / \"data\" / \"molforge\" / \"ECFP4_smiles_checkpoint.pth\"\n",
    "METHOD = \"greedy\"\n",
    "MAX_ROWS = 10000\n",
    "\n",
    "ARGS = SimpleNamespace(\n",
    "    fp = \"ECFP4\",\n",
    "    model_type = \"smiles\",\n",
    "    checkpoint = str(MODEL_PATH),\n",
    "    decode = \"greedy\",\n",
    "    src_vocab_size = 2052,\n",
    "    trg_vocab_size = 109,\n",
    "    src_seq_len = 104,\n",
    "    trg_seq_len = 130,\n",
    "    root_dir = BASE_DIR,\n",
    "    src_sp_prefix = str(SOURCE_SP).replace(\".model\", \"\"),\n",
    "    trg_sp_prefix = str(TARGET_SP).replace(\".model\", \"\"),\n",
    "    rank = \"cpu\",\n",
    "    device = \"cpu\",\n",
    ")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Write settings\n",
    "with open(WORK_DIR / \"settings.txt\", \"w\") as f:\n",
    "    f.write(\"BASE_DIR: \" + str(BASE_DIR) + \"\\n\")\n",
    "    f.write(\"TEST_FILE: \" + str(TEST_FILE) + \"\\n\")\n",
    "    f.write(\"WORK_DIR: \" + str(WORK_DIR) + \"\\n\")\n",
    "    f.write(\"MAX_ROWS: \" + str(MAX_ROWS) + \"\\n\")\n",
    "    for key, value in ARGS.__dict__.items():\n",
    "        f.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 — Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filename = \"data.tsv\"\n",
    "\n",
    "# Data preparation -------------------------------------------------------------\n",
    "\n",
    "# Load test data\n",
    "data = pd.read_csv(TEST_FILE, sep='\\t', nrows=MAX_ROWS)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data.drop(columns=['SMILES_0', 'SIGNATURE', 'SIGNATURE_MORGANS'], inplace=True)\n",
    "\n",
    "# Rename few columns for more clarity\n",
    "data.rename(columns={'SMILES': 'Query SMILES', 'ECFP': 'Query ECFP'}, inplace=True)\n",
    "\n",
    "# Compute ECFP compatible with the MolForge \n",
    "data[\"Query SMILES\"] = data[\"Query SMILES\"].apply(lambda x: x.replace(\" \", \"\"))\n",
    "data[\"Query Mol\"] = data[\"Query SMILES\"].apply(mol_from_smiles)\n",
    "data[\"Query ECFP Object\"] = data[\"Query Mol\"].apply(mol_to_ecfp_molforge)\n",
    "data[\"Query ECFP\"] = data[\"Query ECFP Object\"].apply(ecfp_to_string_molforge)\n",
    "\n",
    "# Clean up the DataFrame\n",
    "data.drop(columns=[\"Query Mol\", \"Query ECFP Object\"], inplace=True)\n",
    "\n",
    "# Save processed data\n",
    "data.to_csv(WORK_DIR / out_filename, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 — Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of src vocab is 2052 and that of trg vocab is 109.\n",
      "Loading checkpoint... ECFP4 smiles\n",
      "Processing 1 / 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tduigou/projects/2024__RetroSynthesis/MolForge/MolForge/predict.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device(args.rank))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 101 / 10000\n",
      "Processing 201 / 10000\n",
      "Processing 301 / 10000\n",
      "Processing 401 / 10000\n",
      "Processing 501 / 10000\n",
      "Processing 601 / 10000\n",
      "Processing 701 / 10000\n",
      "Processing 801 / 10000\n",
      "Processing 901 / 10000\n",
      "Processing 1001 / 10000\n",
      "Processing 1101 / 10000\n",
      "Processing 1201 / 10000\n",
      "Processing 1301 / 10000\n",
      "Processing 1401 / 10000\n",
      "Processing 1501 / 10000\n",
      "Processing 1601 / 10000\n",
      "Processing 1701 / 10000\n",
      "Processing 1801 / 10000\n",
      "Processing 1901 / 10000\n",
      "Processing 2001 / 10000\n",
      "Processing 2101 / 10000\n",
      "Processing 2201 / 10000\n",
      "Processing 2301 / 10000\n",
      "Processing 2401 / 10000\n",
      "Processing 2501 / 10000\n",
      "Processing 2601 / 10000\n",
      "Processing 2701 / 10000\n",
      "Processing 2801 / 10000\n",
      "Processing 2901 / 10000\n",
      "Processing 3001 / 10000\n",
      "Processing 3101 / 10000\n",
      "Processing 3201 / 10000\n",
      "Processing 3301 / 10000\n",
      "Processing 3401 / 10000\n",
      "Processing 3501 / 10000\n",
      "Processing 3601 / 10000\n",
      "Processing 3701 / 10000\n",
      "Processing 3801 / 10000\n",
      "Processing 3901 / 10000\n",
      "Processing 4001 / 10000\n",
      "Processing 4101 / 10000\n",
      "Processing 4201 / 10000\n",
      "Processing 4301 / 10000\n",
      "Processing 4401 / 10000\n",
      "Processing 4501 / 10000\n",
      "Processing 4601 / 10000\n",
      "Processing 4701 / 10000\n",
      "Processing 4801 / 10000\n",
      "Processing 4901 / 10000\n",
      "Processing 5001 / 10000\n",
      "Processing 5101 / 10000\n",
      "Processing 5201 / 10000\n",
      "Processing 5301 / 10000\n",
      "Processing 5401 / 10000\n",
      "Processing 5501 / 10000\n",
      "Processing 5601 / 10000\n",
      "Processing 5701 / 10000\n",
      "Processing 5801 / 10000\n",
      "Processing 5901 / 10000\n",
      "Processing 6001 / 10000\n",
      "Processing 6101 / 10000\n",
      "Processing 6201 / 10000\n",
      "Processing 6301 / 10000\n",
      "Processing 6401 / 10000\n",
      "Processing 6501 / 10000\n",
      "Processing 6601 / 10000\n",
      "Processing 6701 / 10000\n",
      "Processing 6801 / 10000\n",
      "Processing 6901 / 10000\n",
      "Processing 7001 / 10000\n",
      "Processing 7101 / 10000\n",
      "Processing 7201 / 10000\n",
      "Processing 7301 / 10000\n",
      "Processing 7401 / 10000\n",
      "Processing 7501 / 10000\n",
      "Processing 7601 / 10000\n",
      "Processing 7701 / 10000\n",
      "Processing 7801 / 10000\n",
      "Processing 7901 / 10000\n",
      "Processing 8001 / 10000\n",
      "Processing 8101 / 10000\n",
      "Processing 8201 / 10000\n",
      "Processing 8301 / 10000\n",
      "Processing 8401 / 10000\n",
      "Processing 8501 / 10000\n",
      "Processing 8601 / 10000\n",
      "Processing 8701 / 10000\n",
      "Processing 8801 / 10000\n",
      "Processing 8901 / 10000\n",
      "Processing 9001 / 10000\n",
      "Processing 9101 / 10000\n",
      "Processing 9201 / 10000\n",
      "Processing 9301 / 10000\n",
      "Processing 9401 / 10000\n",
      "Processing 9501 / 10000\n",
      "Processing 9601 / 10000\n",
      "Processing 9701 / 10000\n",
      "Processing 9801 / 10000\n",
      "Processing 9901 / 10000\n"
     ]
    }
   ],
   "source": [
    "data_filename = \"data.tsv\"\n",
    "results_filename = \"results.raw.tsv\"\n",
    "\n",
    "# Set up model -------------------------------------------------------------\n",
    "\n",
    "# Load SentencePiece models\n",
    "src_sp = spm.SentencePieceProcessor()\n",
    "src_sp.Load(str(SOURCE_SP))\n",
    "trg_sp = spm.SentencePieceProcessor()\n",
    "trg_sp.Load(str(TARGET_SP))\n",
    "\n",
    "# Load model\n",
    "model = setup(build_model(ARGS).to(ARGS.device), ARGS.checkpoint, ARGS)\n",
    "\n",
    "# Results DataFrame --------------------------------------------------------\n",
    "results = pd.DataFrame(columns=[\"Query ID\", \"Query SMILES\", \"Query ECFP\", \"Predicted SMILES\", \"Time Elapsed\"])\n",
    "\n",
    "# Prediction -------------------------------------------------------------------\n",
    "\n",
    "data = pd.read_csv(WORK_DIR / data_filename, sep=\"\\t\")\n",
    "\n",
    "# Loop over data ECFP\n",
    "for idx, row in data.iterrows():\n",
    "\n",
    "    if idx >= MAX_ROWS:\n",
    "        break\n",
    "    \n",
    "    query_id = idx + 1\n",
    "    query_smiles = row[\"Query SMILES\"]\n",
    "    query_ecfp = row[\"Query ECFP\"]\n",
    "\n",
    "    # Log progress\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processing {query_id} / {len(data)}\")\n",
    "\n",
    "    with handy.Timer() as timer:\n",
    "        # Perform inference\n",
    "        predi_tokens = inference(model, query_ecfp, METHOD, ARGS, src_sp, trg_sp)\n",
    "\n",
    "    predi_smiles = inference(model, query_ecfp, METHOD, ARGS, src_sp, trg_sp)\n",
    "    query_smiles = query_smiles.replace(\" \", \"\")\n",
    "    predi_smiles = predi_smiles.replace(\" \", \"\")\n",
    "    results.loc[idx] = [query_id, query_smiles, query_ecfp, predi_smiles, timer.elapsed]\n",
    "\n",
    "\n",
    "# Save results -----------------------------------------------------------------\n",
    "results.to_csv(WORK_DIR / results_filename, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 — Refine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filename = \"results.raw.tsv\"\n",
    "out_filename = \"results.refined.tsv\"\n",
    "\n",
    "# Load data -------------------------------------------------------------------\n",
    "\n",
    "data = pd.read_csv(WORK_DIR / results_filename, sep=\"\\t\")\n",
    "\n",
    "\n",
    "# Refine data -----------------------------------------------------------------\n",
    "\n",
    "# Let's recompute required information on the Query side\n",
    "data[\"Query SMILES\"] = data[\"Query SMILES\"].apply(remove_spaces)\n",
    "data[\"Query Mol\"] = data[\"Query SMILES\"].apply(mol_from_smiles_with_exception)\n",
    "data[\"Query Counted ECFP Object\"] = data[\"Query Mol\"].apply(mol_to_ecfp)\n",
    "data[\"Query Counted ECFP\"] = data[\"Query Counted ECFP Object\"].apply(ecfp_to_string)\n",
    "data[\"Query Binary ECFP Object\"] = data[\"Query Mol\"].apply(mol_to_ecfp_molforge)\n",
    "data[\"Query Binary ECFP\"] = data[\"Query Binary ECFP Object\"].apply(ecfp_to_string_molforge)\n",
    "\n",
    "# Now let's populate back the prediction side\n",
    "data[\"Predicted Mol\"] = data[\"Predicted SMILES\"].apply(mol_from_smiles_with_exception)\n",
    "data[\"Predicted Counted ECFP Object\"] = data[\"Predicted Mol\"].apply(mol_to_ecfp)\n",
    "data[\"Predicted Counted ECFP\"] = data[\"Predicted Counted ECFP Object\"].apply(ecfp_to_string)\n",
    "data[\"Predicted Binary ECFP Object\"] = data[\"Predicted Mol\"].apply(mol_to_ecfp_molforge)\n",
    "data[\"Predicted Binary ECFP\"] = data[\"Predicted Binary ECFP Object\"].apply(ecfp_to_string_molforge)\n",
    "data[\"Predicted Canonic SMILES\"] = data[\"Predicted Mol\"].apply(mol_to_smiles)\n",
    "\n",
    "# Now let's check for Mol validity\n",
    "data[\"SMILES Syntaxically Valid\"] = data[\"Predicted Mol\"].notnull()\n",
    "\n",
    "# Now let's check for SMILES equality (with and without stereo)\n",
    "data[\"SMILES Exact Match\"] = data[\"Query SMILES\"] == data[\"Predicted Canonic SMILES\"]\n",
    "\n",
    "# Now let's check for Tanimoto identity (with and without stereo)\n",
    "data[\"Tanimoto Counted ECFP\"] = data.apply(lambda x: tanimoto(x[\"Query Counted ECFP Object\"], x[\"Predicted Counted ECFP Object\"]), axis=1)\n",
    "data[\"Tanimoto Counted ECFP Exact Match\"] = data[\"Tanimoto Counted ECFP\"] == 1.0\n",
    "data[\"Tanimoto Binary ECFP\"] = data.apply(lambda x: tanimoto(x[\"Query Binary ECFP Object\"], x[\"Predicted Binary ECFP Object\"]), axis=1)\n",
    "data[\"Tanimoto Binary ECFP Exact Match\"] = data[\"Tanimoto Binary ECFP\"] == 1.0\n",
    "\n",
    "# Finally export the refined DataFrame\n",
    "cols = [\n",
    "    \"Query ID\",\n",
    "    \"Query SMILES\",\n",
    "    \"Query Counted ECFP\",\n",
    "    \"Query Binary ECFP\",\n",
    "    \"Predicted SMILES\",\n",
    "    \"Predicted Counted ECFP\",\n",
    "    \"Predicted Binary ECFP\",\n",
    "    \"Predicted Canonic SMILES\",\n",
    "    \"Tanimoto Counted ECFP\",\n",
    "    \"Tanimoto Counted ECFP Exact Match\",\n",
    "    \"Tanimoto Binary ECFP\",\n",
    "    \"Tanimoto Binary ECFP Exact Match\",\n",
    "    \"SMILES Exact Match\",\n",
    "    \"SMILES Syntaxically Valid\",\n",
    "    \"Time Elapsed\",\n",
    "]\n",
    "data.to_csv(WORK_DIR / out_filename, sep=\"\\t\", index=False, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 — Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats:\n",
      "                             Stat     Value\n",
      "0                 SMILES Accuracy  0.648700\n",
      "1  Tanimoto Counted ECFP Accuracy  0.673700\n",
      "2          SMILES Syntax Validity  0.998200\n",
      "3   Tanimoto Binary ECFP Accuracy  0.913000\n",
      "4            Average Time Elapsed  1.470123\n",
      "\n",
      "uniqueness:\n",
      "  Distinct Molecules per Query  Count\n",
      "0                            0   3263\n",
      "1                            1   6737\n"
     ]
    }
   ],
   "source": [
    "results_filename = \"results.refined.tsv\"\n",
    "top_k = 1\n",
    "\n",
    "from handy import get_summary, get_statistics, get_uniqueness\n",
    "\n",
    "\n",
    "# Load data -------------------------------------------------------------------\n",
    "\n",
    "data = pd.read_csv(WORK_DIR / results_filename, sep=\"\\t\")\n",
    "\n",
    "# Add dummy \"Predicted Log Prob\" column if not present (for compatibility)\n",
    "if \"Predicted Log Prob\" not in data.columns:\n",
    "    data[\"Predicted Log Prob\"] = 0.0\n",
    "\n",
    "\n",
    "# Summary ---------------------------------------------------------------------\n",
    "\n",
    "summary = get_summary(data, topk=top_k)\n",
    "out_filename = \"results.summary.tsv\"\n",
    "summary.to_csv(WORK_DIR / out_filename, sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "# Statistics -------------------------------------------------------------------\n",
    "\n",
    "stats = get_statistics(data, topk=top_k)\n",
    "out_filename = \"results.statistics.tsv\"\n",
    "stats.to_csv(WORK_DIR / out_filename, sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "# Uniqueness -------------------------------------------------------------------\n",
    "\n",
    "uniqueness = get_uniqueness(data, topk=top_k)\n",
    "out_filename = \"results.uniqueness.tsv\"\n",
    "uniqueness.to_csv(WORK_DIR / out_filename, sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "print(f\"stats:\")\n",
    "print(stats)\n",
    "print()\n",
    "print(f\"uniqueness:\")\n",
    "print(uniqueness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 — Pass MolForge's test set through MolForge's model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 — Init & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "from rdkit import RDLogger\n",
    "\n",
    "import handy\n",
    "from paper.dataset.utils import (\n",
    "    mol_from_smiles,\n",
    "    mol_to_smiles,\n",
    "    tanimoto,\n",
    "    mol_to_ecfp,\n",
    "    ecfp_to_string,\n",
    ")\n",
    "from handy import mol_to_ecfp_molforge, ecfp_to_string_molforge\n",
    "from MolForge.utils import pad_or_truncate, pad_id, build_model\n",
    "from MolForge.predict import greedy_search, beam_search, setup\n",
    "\n",
    "# Logging --------------------------------------------------------------------\n",
    "\n",
    "RDLogger.DisableLog(\"rdApp.error\")\n",
    "RDLogger.DisableLog('rdApp.warning')\n",
    "\n",
    "\n",
    "# Utils ------------------------------------------------------------------------\n",
    "\n",
    "def remove_spaces(x):\n",
    "    return x.replace(\" \", \"\")\n",
    "\n",
    "\n",
    "def inference(model, input_sentence, method, args, src_sp, trg_sp, return_attn=False):\n",
    "\n",
    "    tokenized = src_sp.EncodeAsIds(input_sentence)\n",
    "    src_data = torch.LongTensor(pad_or_truncate(tokenized, args.src_seq_len)).unsqueeze(0).to(args.device) # (1, L)\n",
    "    e_mask = (src_data != pad_id).unsqueeze(1).to(args.device) # (1, 1, L)\n",
    "\n",
    "    model.eval()\n",
    "    src_data = model.src_embedding(src_data)\n",
    "    src_data = model.src_positional_encoder(src_data)\n",
    "    e_output = model.encoder(src_data, e_mask) # (1, L, d_model)\n",
    "\n",
    "    if method == 'greedy':\n",
    "        result, attn = greedy_search(model, e_output, e_mask, trg_sp, args.device, True)\n",
    "\n",
    "    elif method == 'beam':\n",
    "        result, attn = beam_search(model, e_output, e_mask, trg_sp, args.device, True)\n",
    "\n",
    "    if return_attn:\n",
    "        return result, attn\n",
    "    \n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "def mol_from_smiles_with_exception(mol):\n",
    "    try:\n",
    "        return mol_from_smiles(mol)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def mol_from_smiles_no_stereo(smiles):\n",
    "    \"\"\"Convert SMILES to RDKit Mol object without stereo information.\"\"\"\n",
    "    try:\n",
    "        return mol_from_smiles(smiles, clear_stereo=True)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Settings --------------------------------------------------------------------\n",
    "\n",
    "BASE_DIR = Path().resolve().parent\n",
    "TEST_FILE = BASE_DIR / 'data' / 'molforge' / 'ECFP4.smiles.test'\n",
    "WORK_DIR = BASE_DIR / 'notebooks' / 'molforge' / 'case-4'\n",
    "\n",
    "SOURCE_SP = BASE_DIR / \"data\" / \"molforge\" / \"ECFP4_vocab_sp.model\"\n",
    "TARGET_SP = BASE_DIR / \"data\" / \"molforge\" / \"smiles_vocab_sp.model\"\n",
    "\n",
    "MODEL_PATH = BASE_DIR / \"data\" / \"molforge\" / \"ECFP4_smiles_checkpoint.pth\"\n",
    "METHOD = \"greedy\"\n",
    "MAX_ROWS = 100\n",
    "\n",
    "ARGS = SimpleNamespace(\n",
    "    fp = \"ECFP4\",\n",
    "    model_type = \"smiles\",\n",
    "    checkpoint = str(MODEL_PATH),\n",
    "    decode = \"greedy\",\n",
    "    src_vocab_size = 2052,\n",
    "    trg_vocab_size = 109,\n",
    "    src_seq_len = 104,\n",
    "    trg_seq_len = 130,\n",
    "    root_dir = BASE_DIR,\n",
    "    src_sp_prefix = str(SOURCE_SP).replace(\".model\", \"\"),\n",
    "    trg_sp_prefix = str(TARGET_SP).replace(\".model\", \"\"),\n",
    "    rank = \"cpu\",\n",
    "    device = \"cpu\",\n",
    ")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Write settings\n",
    "with open(WORK_DIR / \"settings.txt\", \"w\") as f:\n",
    "    f.write(\"BASE_DIR: \" + str(BASE_DIR) + \"\\n\")\n",
    "    f.write(\"TEST_FILE: \" + str(TEST_FILE) + \"\\n\")\n",
    "    f.write(\"WORK_DIR: \" + str(WORK_DIR) + \"\\n\")\n",
    "    f.write(\"MAX_ROWS: \" + str(MAX_ROWS) + \"\\n\")\n",
    "    for key, value in ARGS.__dict__.items():\n",
    "        f.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 — Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filename = \"data.tsv\"\n",
    "\n",
    "# Data preparation -------------------------------------------------------------\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(TEST_FILE, sep=\"\\t\", header=None, nrows=MAX_ROWS)\n",
    "data.rename(columns={0: \"Query SMILES\", 1: \"Query ECFP\"}, inplace=True)\n",
    "\n",
    "# Remove spaces in SMILES\n",
    "data[\"Query SMILES\"] = data[\"Query SMILES\"].apply(remove_spaces)\n",
    "\n",
    "# Append a \"Query ID\" column containing the row number for easier reference\n",
    "data['Query ID'] = range(1, len(data) + 1)\n",
    "\n",
    "# Reorder columns to have \"Query ID\" first\n",
    "cols = data.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "data = data[cols]\n",
    "\n",
    "# Save the processed data\n",
    "data.to_csv(WORK_DIR / out_filename, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 — Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of src vocab is 2052 and that of trg vocab is 109.\n",
      "Loading checkpoint... ECFP4 smiles\n",
      "Processing 1 / 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tduigou/projects/2024__RetroSynthesis/MolForge/MolForge/predict.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device(args.rank))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 101 / 10000\n",
      "Processing 201 / 10000\n",
      "Processing 301 / 10000\n",
      "Processing 401 / 10000\n",
      "Processing 501 / 10000\n",
      "Processing 601 / 10000\n",
      "Processing 701 / 10000\n",
      "Processing 801 / 10000\n",
      "Processing 901 / 10000\n",
      "Processing 1001 / 10000\n",
      "Processing 1101 / 10000\n",
      "Processing 1201 / 10000\n",
      "Processing 1301 / 10000\n",
      "Processing 1401 / 10000\n",
      "Processing 1501 / 10000\n",
      "Processing 1601 / 10000\n",
      "Processing 1701 / 10000\n",
      "Processing 1801 / 10000\n",
      "Processing 1901 / 10000\n",
      "Processing 2001 / 10000\n",
      "Processing 2101 / 10000\n",
      "Processing 2201 / 10000\n",
      "Processing 2301 / 10000\n",
      "Processing 2401 / 10000\n",
      "Processing 2501 / 10000\n",
      "Processing 2601 / 10000\n",
      "Processing 2701 / 10000\n",
      "Processing 2801 / 10000\n",
      "Processing 2901 / 10000\n",
      "Processing 3001 / 10000\n",
      "Processing 3101 / 10000\n",
      "Processing 3201 / 10000\n",
      "Processing 3301 / 10000\n",
      "Processing 3401 / 10000\n",
      "Processing 3501 / 10000\n",
      "Processing 3601 / 10000\n",
      "Processing 3701 / 10000\n",
      "Processing 3801 / 10000\n",
      "Processing 3901 / 10000\n",
      "Processing 4001 / 10000\n",
      "Processing 4101 / 10000\n",
      "Processing 4201 / 10000\n",
      "Processing 4301 / 10000\n",
      "Processing 4401 / 10000\n",
      "Processing 4501 / 10000\n",
      "Processing 4601 / 10000\n",
      "Processing 4701 / 10000\n",
      "Processing 4801 / 10000\n",
      "Processing 4901 / 10000\n",
      "Processing 5001 / 10000\n",
      "Processing 5101 / 10000\n",
      "Processing 5201 / 10000\n",
      "Processing 5301 / 10000\n",
      "Processing 5401 / 10000\n",
      "Processing 5501 / 10000\n",
      "Processing 5601 / 10000\n",
      "Processing 5701 / 10000\n",
      "Processing 5801 / 10000\n",
      "Processing 5901 / 10000\n",
      "Processing 6001 / 10000\n",
      "Processing 6101 / 10000\n",
      "Processing 6201 / 10000\n",
      "Processing 6301 / 10000\n",
      "Processing 6401 / 10000\n",
      "Processing 6501 / 10000\n",
      "Processing 6601 / 10000\n",
      "Processing 6701 / 10000\n",
      "Processing 6801 / 10000\n",
      "Processing 6901 / 10000\n",
      "Processing 7001 / 10000\n",
      "Processing 7101 / 10000\n",
      "Processing 7201 / 10000\n",
      "Processing 7301 / 10000\n",
      "Processing 7401 / 10000\n",
      "Processing 7501 / 10000\n",
      "Processing 7601 / 10000\n",
      "Processing 7701 / 10000\n",
      "Processing 7801 / 10000\n",
      "Processing 7901 / 10000\n",
      "Processing 8001 / 10000\n",
      "Processing 8101 / 10000\n",
      "Processing 8201 / 10000\n",
      "Processing 8301 / 10000\n",
      "Processing 8401 / 10000\n",
      "Processing 8501 / 10000\n",
      "Processing 8601 / 10000\n",
      "Processing 8701 / 10000\n",
      "Processing 8801 / 10000\n",
      "Processing 8901 / 10000\n",
      "Processing 9001 / 10000\n",
      "Processing 9101 / 10000\n",
      "Processing 9201 / 10000\n",
      "Processing 9301 / 10000\n",
      "Processing 9401 / 10000\n",
      "Processing 9501 / 10000\n",
      "Processing 9601 / 10000\n",
      "Processing 9701 / 10000\n",
      "Processing 9801 / 10000\n",
      "Processing 9901 / 10000\n"
     ]
    }
   ],
   "source": [
    "data_filename = \"data.tsv\"\n",
    "results_filename = \"results.raw.tsv\"\n",
    "\n",
    "# Set up model -------------------------------------------------------------\n",
    "\n",
    "# Load SentencePiece models\n",
    "src_sp = spm.SentencePieceProcessor()\n",
    "src_sp.Load(str(SOURCE_SP))\n",
    "trg_sp = spm.SentencePieceProcessor()\n",
    "trg_sp.Load(str(TARGET_SP))\n",
    "\n",
    "# Load model\n",
    "model = setup(build_model(ARGS).to(ARGS.device), ARGS.checkpoint, ARGS)\n",
    "\n",
    "# Results DataFrame ----------------------------------------------------------\n",
    "\n",
    "results = pd.DataFrame(columns=[\"Query ID\", \"Query SMILES\", \"Query ECFP\", \"Predicted SMILES\", \"Time Elapsed\"])\n",
    "\n",
    "# Load data ------------------------------------------------------------------\n",
    "\n",
    "data = pd.read_csv(WORK_DIR / data_filename, sep=\"\\t\")\n",
    "\n",
    "# Prediction -----------------------------------------------------------------\n",
    "\n",
    "# Loop over data ECFP\n",
    "for idx, row in data.iterrows():\n",
    "\n",
    "    if idx >= MAX_ROWS:\n",
    "        break\n",
    "    \n",
    "    query_id = idx + 1\n",
    "    query_smiles = row[\"Query SMILES\"]\n",
    "    query_ecfp = row[\"Query ECFP\"]\n",
    "\n",
    "    # Log progress\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processing {query_id} / {len(data)}\")\n",
    "\n",
    "    with handy.Timer() as timer:\n",
    "        # Perform inference\n",
    "        predi_tokens = inference(model, query_ecfp, METHOD, ARGS, src_sp, trg_sp)\n",
    "\n",
    "    predi_smiles = inference(model, query_ecfp, METHOD, ARGS, src_sp, trg_sp)\n",
    "    # query_smiles = query_smiles.replace(\" \", \"\")\n",
    "    predi_smiles = predi_smiles.replace(\" \", \"\")\n",
    "    results.loc[idx] = [query_id, query_smiles, query_ecfp, predi_smiles, timer.elapsed]\n",
    "\n",
    "\n",
    "# Save results -----------------------------------------------------------------\n",
    "results.to_csv(WORK_DIR / results_filename, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Refine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filename = \"results.raw.tsv\"\n",
    "out_filename = \"results.refined.tsv\"\n",
    "\n",
    "# Load data -------------------------------------------------------------------\n",
    "\n",
    "data = pd.read_csv(WORK_DIR / results_filename, sep=\"\\t\")\n",
    "\n",
    "\n",
    "# Refine data -----------------------------------------------------------------\n",
    "\n",
    "# Let's recompute required information on the Query side\n",
    "data[\"Query SMILES\"] = data[\"Query SMILES\"].apply(remove_spaces)\n",
    "data[\"Query Mol\"] = data[\"Query SMILES\"].apply(mol_from_smiles_with_exception)\n",
    "data[\"Query Counted ECFP Object\"] = data[\"Query Mol\"].apply(mol_to_ecfp)\n",
    "data[\"Query Counted ECFP\"] = data[\"Query Counted ECFP Object\"].apply(ecfp_to_string)\n",
    "data[\"Query Binary ECFP Object\"] = data[\"Query Mol\"].apply(mol_to_ecfp_molforge)\n",
    "data[\"Query Binary ECFP\"] = data[\"Query Binary ECFP Object\"].apply(ecfp_to_string_molforge)\n",
    "\n",
    "# Now let's populate back the prediction side\n",
    "data[\"Predicted Mol\"] = data[\"Predicted SMILES\"].apply(mol_from_smiles_with_exception)\n",
    "data[\"Predicted Counted ECFP Object\"] = data[\"Predicted Mol\"].apply(mol_to_ecfp)\n",
    "data[\"Predicted Counted ECFP\"] = data[\"Predicted Counted ECFP Object\"].apply(ecfp_to_string)\n",
    "data[\"Predicted Binary ECFP Object\"] = data[\"Predicted Mol\"].apply(mol_to_ecfp_molforge)\n",
    "data[\"Predicted Binary ECFP\"] = data[\"Predicted Binary ECFP Object\"].apply(ecfp_to_string_molforge)\n",
    "data[\"Predicted Canonic SMILES\"] = data[\"Predicted Mol\"].apply(mol_to_smiles)\n",
    "\n",
    "# Now let's check for Mol validity\n",
    "data[\"SMILES Syntaxically Valid\"] = data[\"Predicted Mol\"].notnull()\n",
    "\n",
    "# Now let's check for SMILES equality\n",
    "data[\"SMILES Exact Match\"] = data[\"Query SMILES\"] == data[\"Predicted SMILES\"]\n",
    "\n",
    "# Now let's check for Tanimoto identity\n",
    "data[\"Tanimoto Counted ECFP\"] = data.apply(lambda x: tanimoto(x[\"Query Counted ECFP Object\"], x[\"Predicted Counted ECFP Object\"]), axis=1)\n",
    "data[\"Tanimoto Counted ECFP Exact Match\"] = data[\"Tanimoto Counted ECFP\"] == 1.0\n",
    "data[\"Tanimoto Binary ECFP\"] = data.apply(lambda x: tanimoto(x[\"Query Binary ECFP Object\"], x[\"Predicted Binary ECFP Object\"]), axis=1)\n",
    "data[\"Tanimoto Binary ECFP Exact Match\"] = data[\"Tanimoto Binary ECFP\"] == 1.0\n",
    "\n",
    "# Finally export the refined DataFrame\n",
    "cols = [\n",
    "    \"Query ID\",\n",
    "    \"Query SMILES\",\n",
    "    \"Query Counted ECFP\",\n",
    "    \"Query Binary ECFP\",\n",
    "    \"Predicted SMILES\",\n",
    "    \"Predicted Counted ECFP\",\n",
    "    \"Predicted Binary ECFP\",\n",
    "    \"Predicted Canonic SMILES\",\n",
    "    \"Tanimoto Counted ECFP\",\n",
    "    \"Tanimoto Counted ECFP Exact Match\",\n",
    "    \"Tanimoto Binary ECFP\",\n",
    "    \"Tanimoto Binary ECFP Exact Match\",\n",
    "    \"SMILES Exact Match\",\n",
    "    \"SMILES Syntaxically Valid\",\n",
    "    \"Time Elapsed\",\n",
    "]\n",
    "data.to_csv(WORK_DIR / out_filename, sep=\"\\t\", index=False, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 — Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats:\n",
      "                             Stat     Value\n",
      "0                 SMILES Accuracy  0.604800\n",
      "1  Tanimoto Counted ECFP Accuracy  0.667300\n",
      "2          SMILES Syntax Validity  0.997700\n",
      "3   Tanimoto Binary ECFP Accuracy  0.871600\n",
      "4            Average Time Elapsed  1.715669\n",
      "\n",
      "uniqueness:\n",
      "  Distinct Molecules per Query  Count\n",
      "0                            0   3327\n",
      "1                            1   6673\n"
     ]
    }
   ],
   "source": [
    "results_filename = \"results.refined.tsv\"\n",
    "top_k = 1\n",
    "\n",
    "from handy import get_summary, get_statistics, get_uniqueness\n",
    "\n",
    "\n",
    "# Load data -------------------------------------------------------------------\n",
    "\n",
    "data = pd.read_csv(WORK_DIR / results_filename, sep=\"\\t\")\n",
    "\n",
    "# Add dummy \"Predicted Log Prob\" column if not present (for compatibility)\n",
    "if \"Predicted Log Prob\" not in data.columns:\n",
    "    data[\"Predicted Log Prob\"] = 0.0\n",
    "\n",
    "\n",
    "# Summary ---------------------------------------------------------------------\n",
    "\n",
    "summary = get_summary(data, topk=top_k)\n",
    "out_filename = \"results.summary.tsv\"\n",
    "summary.to_csv(WORK_DIR / out_filename, sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "# Statistics -------------------------------------------------------------------\n",
    "\n",
    "stats = get_statistics(data, topk=top_k)\n",
    "out_filename = \"results.statistics.tsv\"\n",
    "stats.to_csv(WORK_DIR / out_filename, sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "# Uniqueness -------------------------------------------------------------------\n",
    "\n",
    "uniqueness = get_uniqueness(data, topk=top_k)\n",
    "out_filename = \"results.uniqueness.tsv\"\n",
    "uniqueness.to_csv(WORK_DIR / out_filename, sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "print(f\"stats:\")\n",
    "print(stats)\n",
    "print()\n",
    "print(f\"uniqueness:\")\n",
    "print(uniqueness)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signature-paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
